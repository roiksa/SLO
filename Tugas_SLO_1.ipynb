{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (system-wide)",
      "language": "python",
      "metadata": {
        "cocalc": {
          "description": "Python 3 programming language",
          "priority": 100,
          "url": "https://www.python.org/"
        }
      },
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Tugas SLO 1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ExhmHl7Q5t7_"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcucdHxMqi5L"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDB9UOyp8TVw",
        "outputId": "ad278b7c-bd0f-40f2-a736-d1c7c82e5990"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_uZVJpWDO0u",
        "outputId": "81f070c2-069a-434e-a88c-0b7d9f042ac0"
      },
      "source": [
        "cd /content/drive/MyDrive/SLO/SLO-main/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/SLO/SLO-main\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IkPXCwDgRJC"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "# from sklearn.datasets import load_iris"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tAnSVBc4lor"
      },
      "source": [
        "# from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "ZLsTWZHS4n3I",
        "outputId": "179c7fcb-c6f5-478f-bd06-c46405b73226"
      },
      "source": [
        "df = pd.read_csv('data_cancer.csv')\n",
        "# df = pd.read_excel(\"xxx.xls\", sheet_name=\"sheet 1\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302         M  ...                  0.11890          NaN\n",
              "1    842517         M  ...                  0.08902          NaN\n",
              "2  84300903         M  ...                  0.08758          NaN\n",
              "3  84348301         M  ...                  0.17300          NaN\n",
              "4  84358402         M  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EvzBkNa6wV5",
        "outputId": "3032afac-0045-4674-cf06-663f2d7ee4d1"
      },
      "source": [
        "test_X = df.head()\n",
        "print(test_X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 33)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwUdBnpI4fMX"
      },
      "source": [
        "X = df.iloc[:,2:32].values\n",
        "Y = df.iloc[:,[1]].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwM9k-xaER1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80148032-fe28-4920-cec5-f29ecc11cebf"
      },
      "source": [
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['B']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['M']\n",
            " ['B']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIpKU3x85Z5y",
        "outputId": "9a589583-0062-4d56-e4bd-01de61ddf0e6"
      },
      "source": [
        "target_names = df['diagnosis'].unique()\n",
        "print(target_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['M' 'B']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_xTqq4V7q7J",
        "outputId": "5cecc88a-c250-4dd8-f314-7c9296c15274"
      },
      "source": [
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 30)\n",
            "(569, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLHeiyhI4VCT",
        "outputId": "946c6fb6-2a5a-4a03-b689-45b89a0a2a92"
      },
      "source": [
        "(trainX, testX, trainY, testY) = train_test_split(X, Y, random_state=3, test_size=0.2)\n",
        "print(trainX.shape)\n",
        "print(testX.shape)\n",
        "print(trainY.shape)\n",
        "print(testY.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(455, 30)\n",
            "(114, 30)\n",
            "(455, 1)\n",
            "(114, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6vqEgqCgRJf"
      },
      "source": [
        "# define the dictionary of models our script can use, where the key\n",
        "# to the dictionary is the name of the model (supplied via command\n",
        "# line argument) and the value is the model itself\n",
        "models = {\n",
        "\t\"knn\": KNeighborsClassifier(n_neighbors=3),\n",
        "\t\"naive_bayes\": GaussianNB(),\n",
        "\t\"logit\": LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\"),\n",
        "\t\"svm\": SVC(kernel=\"rbf\", gamma=\"auto\"),\n",
        "\t\"decision_tree\": DecisionTreeClassifier(),\n",
        "\t\"random_forest\": RandomForestClassifier(n_estimators=100),\n",
        "\t\"mlp\": MLPClassifier()\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAmEPiyMgRJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf3d265-b7f8-41fb-ad94-599ce41358fc"
      },
      "source": [
        "# train the model\n",
        "print(\"[INFO] using '{}' model\".format(\"knn\"))\n",
        "model = models[\"knn\"]\n",
        "model.fit(trainX, trainY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] using 'knn' model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMst7JNbgRJo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd472c67-7a5a-41a6-b12c-8feebd518314"
      },
      "source": [
        "# make predictions on our data and show a classification report\n",
        "print(\"[INFO] evaluating...\")\n",
        "predictions = model.predict(testX)\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating...\n",
            "['B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B'\n",
            " 'M' 'B' 'B' 'B' 'B' 'M' 'B' 'B' 'B' 'M' 'M' 'M' 'M' 'B' 'M' 'B' 'B' 'B'\n",
            " 'B' 'B' 'M' 'M' 'M' 'B' 'B' 'M' 'B' 'B' 'B' 'M' 'M' 'B' 'M' 'B' 'B' 'M'\n",
            " 'M' 'B' 'B' 'M' 'B' 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'M' 'B' 'M' 'M' 'B' 'B'\n",
            " 'M' 'M' 'B' 'M' 'B' 'M' 'B' 'M' 'M' 'B' 'B' 'B' 'M' 'M' 'B' 'B' 'B' 'B'\n",
            " 'M' 'M' 'M' 'B' 'B' 'M' 'B' 'M' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'M' 'M' 'B'\n",
            " 'M' 'B' 'B' 'M' 'B' 'B']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Puym9l3iF4LO",
        "outputId": "d6fc0f36-8ce1-4c5d-f733-1f17bed8490b"
      },
      "source": [
        "print(classification_report(testY, predictions,\n",
        "\ttarget_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.95      0.95      0.95        74\n",
            "           B       0.90      0.90      0.90        40\n",
            "\n",
            "    accuracy                           0.93       114\n",
            "   macro avg       0.92      0.92      0.92       114\n",
            "weighted avg       0.93      0.93      0.93       114\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaVoaNHOgRJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c8957c-60fe-46fa-849a-127686c1f82a"
      },
      "source": [
        "# train the model\n",
        "print(\"[INFO] using '{}' model\".format(\"naive_bayes\"))\n",
        "model = models[\"naive_bayes\"]\n",
        "model.fit(trainX, trainY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] using 'naive_bayes' model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:206: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcB3GEAGgRJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d080442e-be59-41a9-c88d-8dfa66e36856"
      },
      "source": [
        "# make predictions on our data and show a classification report\n",
        "print(\"[INFO] evaluating...\")\n",
        "predictions = model.predict(testX)\n",
        "print(classification_report(testY, predictions,\n",
        "\ttarget_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.93      0.96      0.95        74\n",
            "           B       0.92      0.88      0.90        40\n",
            "\n",
            "    accuracy                           0.93       114\n",
            "   macro avg       0.93      0.92      0.92       114\n",
            "weighted avg       0.93      0.93      0.93       114\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag3ekIj7gRJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f29e341-8efe-486d-d569-f112aa3191c3"
      },
      "source": [
        "# train the model\n",
        "print(\"[INFO] using '{}' model\".format(\"logit\"))\n",
        "model = models[\"logit\"]\n",
        "model.fit(trainX, trainY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] using 'logit' model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ_5q-Y6gRJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4cc440c-66aa-45a7-da93-da1e4a1470cb"
      },
      "source": [
        "# make predictions on our data and show a classification report\n",
        "print(\"[INFO] evaluating...\")\n",
        "predictions = model.predict(testX)\n",
        "print(classification_report(testY, predictions,\n",
        "\ttarget_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.92      0.95      0.93        74\n",
            "           B       0.89      0.85      0.87        40\n",
            "\n",
            "    accuracy                           0.91       114\n",
            "   macro avg       0.91      0.90      0.90       114\n",
            "weighted avg       0.91      0.91      0.91       114\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpMbga3dgRJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b899589b-ab21-455e-816d-c1056afad959"
      },
      "source": [
        "# train the model\n",
        "print(\"[INFO] using '{}' model\".format(\"svm\"))\n",
        "model = models[\"svm\"]\n",
        "model.fit(trainX, trainY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] using 'svm' model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ouv_k7vsgRJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9ae12e5-bce5-4f3c-ce73-5f021b24b5e3"
      },
      "source": [
        "# make predictions on our data and show a classification report\n",
        "print(\"[INFO] evaluating...\")\n",
        "predictions = model.predict(testX)\n",
        "print(classification_report(testY, predictions,\n",
        "\ttarget_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.65      1.00      0.79        74\n",
            "           B       0.00      0.00      0.00        40\n",
            "\n",
            "    accuracy                           0.65       114\n",
            "   macro avg       0.32      0.50      0.39       114\n",
            "weighted avg       0.42      0.65      0.51       114\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl-fW3p9gRJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c7317c-5793-49bf-8c31-1bda8a329040"
      },
      "source": [
        "# train the model\n",
        "print(\"[INFO] using '{}' model\".format(\"decision_tree\"))\n",
        "model = models[\"decision_tree\"]\n",
        "model.fit(trainX, trainY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] using 'decision_tree' model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM2l7Dl4gRJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51b2e42-9cc4-4879-9d15-7e2273d049b7"
      },
      "source": [
        "# make predictions on our data and show a classification report\n",
        "print(\"[INFO] evaluating...\")\n",
        "predictions = model.predict(testX)\n",
        "print(classification_report(testY, predictions,\n",
        "\ttarget_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.94      0.92      0.93        74\n",
            "           B       0.86      0.90      0.88        40\n",
            "\n",
            "    accuracy                           0.91       114\n",
            "   macro avg       0.90      0.91      0.90       114\n",
            "weighted avg       0.91      0.91      0.91       114\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1Qtpf0UgRJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90b2264e-1827-4d6f-d6c8-7e484be7d8f2"
      },
      "source": [
        "# train the model\n",
        "print(\"[INFO] using '{}' model\".format(\"random_forest\"))\n",
        "model = models[\"random_forest\"]\n",
        "model.fit(trainX, trainY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] using 'random_forest' model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fohWRtxcgRKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86a5eff9-2341-4e08-9466-cb9e3fe1cae8"
      },
      "source": [
        "# make predictions on our data and show a classification report\n",
        "print(\"[INFO] evaluating...\")\n",
        "predictions = model.predict(testX)\n",
        "print(classification_report(testY, predictions,\n",
        "\ttarget_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.96      0.95      0.95        74\n",
            "           B       0.90      0.93      0.91        40\n",
            "\n",
            "    accuracy                           0.94       114\n",
            "   macro avg       0.93      0.94      0.93       114\n",
            "weighted avg       0.94      0.94      0.94       114\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNdRh3_8gRKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426d5cdb-40cb-4e91-b94b-423b3e90b9c1"
      },
      "source": [
        "# train the model\n",
        "print(\"[INFO] using '{}' model\".format(\"mlp\"))\n",
        "model = models[\"mlp\"]\n",
        "model.fit(trainX, trainY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] using 'mlp' model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:934: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtSKouEIgRKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2470b046-2ab0-4f7f-8991-8061656fcc16"
      },
      "source": [
        "# make predictions on our data and show a classification report\n",
        "print(\"[INFO] evaluating...\")\n",
        "predictions = model.predict(testX)\n",
        "print(classification_report(testY, predictions,\n",
        "\ttarget_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.94      0.97      0.95        74\n",
            "           B       0.95      0.88      0.91        40\n",
            "\n",
            "    accuracy                           0.94       114\n",
            "   macro avg       0.94      0.92      0.93       114\n",
            "weighted avg       0.94      0.94      0.94       114\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExhmHl7Q5t7_"
      },
      "source": [
        "# **Artificial Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu7aptJJgRKE"
      },
      "source": [
        "# import the necessary packages\n",
        "from keras.models import Sequential\n",
        "# from keras.layers.convolutional import Conv2D\n",
        "# from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "# from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import classification_report\n",
        "# from PIL import Image\n",
        "# from imutils import paths\n",
        "# import numpy as np\n",
        "# import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH24XaG3AnPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ebf593-01b9-475d-fc49-2f1482d049da"
      },
      "source": [
        "Y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['B'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['M'],\n",
              "       ['B']], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "897YXSjv55ZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b122da-d755-467f-dbbd-a2d1859096a8"
      },
      "source": [
        "# encode the labels, converting them from strings to integers\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(Y)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pcXvXBq59PY",
        "outputId": "2a8d3543-24ce-4881-9727-6232ef43fbb2"
      },
      "source": [
        "(trainX, testX, trainY, testY) = train_test_split(X,\tnp.array(labels), test_size=0.2)\n",
        "print(trainX.shape)\n",
        "print(testX.shape)\n",
        "print(trainY.shape)\n",
        "print(testY.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(455, 30)\n",
            "(114, 30)\n",
            "(455, 1)\n",
            "(114, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73GGlwI76EkY"
      },
      "source": [
        "#model ANN\n",
        "\n",
        "model = Sequential(\n",
        "    [\n",
        "        Dense(32, activation=\"relu\", input_shape=(30,)), #hidden layer 1\n",
        "        Dense(16, activation=\"relu\"), #hidden layer 2\n",
        "        Dense(16, activation=\"relu\"),\n",
        "        Dense(8, activation=\"relu\"),\n",
        "        Dense(1, activation = \"sigmoid\"),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWptPSzZ6V-_",
        "outputId": "daab07e7-0d33-4b0a-cf82-792281f6ff09"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32)                992       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 1,937\n",
            "Trainable params: 1,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IoP9cED60mA",
        "outputId": "b0f272b0-3798-4a45-d5af-0727012a77f5"
      },
      "source": [
        "# train the model using the Adam optimizer\n",
        "print(\"[INFO] training network...\")\n",
        "opt = Adam(lr=1e-3, decay=1e-3 / 50)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt, #2 kelas --> binary_crossentropy\n",
        "\tmetrics=[\"accuracy\"])\n",
        "H = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=1000, batch_size=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] training network...\n",
            "Epoch 1/1000\n",
            "23/23 [==============================] - 1s 23ms/step - loss: 7.1280 - accuracy: 0.4473 - val_loss: 0.8696 - val_accuracy: 0.3860\n",
            "Epoch 2/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.5811 - accuracy: 0.6772 - val_loss: 0.3852 - val_accuracy: 0.8596\n",
            "Epoch 3/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3511 - accuracy: 0.8988 - val_loss: 0.3352 - val_accuracy: 0.9035\n",
            "Epoch 4/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.9083 - val_loss: 0.4086 - val_accuracy: 0.8246\n",
            "Epoch 5/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3833 - accuracy: 0.8337 - val_loss: 0.2976 - val_accuracy: 0.9123\n",
            "Epoch 6/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2943 - accuracy: 0.9105 - val_loss: 0.2785 - val_accuracy: 0.9035\n",
            "Epoch 7/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2670 - accuracy: 0.9115 - val_loss: 0.2923 - val_accuracy: 0.8947\n",
            "Epoch 8/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2597 - accuracy: 0.9052 - val_loss: 0.2807 - val_accuracy: 0.9123\n",
            "Epoch 9/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3143 - accuracy: 0.8792 - val_loss: 0.2693 - val_accuracy: 0.9035\n",
            "Epoch 10/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2487 - accuracy: 0.9136 - val_loss: 0.3319 - val_accuracy: 0.8596\n",
            "Epoch 11/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8702 - val_loss: 0.2772 - val_accuracy: 0.9123\n",
            "Epoch 12/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.3321 - accuracy: 0.8597 - val_loss: 0.2423 - val_accuracy: 0.9123\n",
            "Epoch 13/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2194 - accuracy: 0.9177 - val_loss: 0.2586 - val_accuracy: 0.8947\n",
            "Epoch 14/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2428 - accuracy: 0.9121 - val_loss: 0.2428 - val_accuracy: 0.9211\n",
            "Epoch 15/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2284 - accuracy: 0.9095 - val_loss: 0.2912 - val_accuracy: 0.8947\n",
            "Epoch 16/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2748 - accuracy: 0.8821 - val_loss: 0.2369 - val_accuracy: 0.9035\n",
            "Epoch 17/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2162 - accuracy: 0.9073 - val_loss: 0.2383 - val_accuracy: 0.9035\n",
            "Epoch 18/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2014 - accuracy: 0.9282 - val_loss: 0.2362 - val_accuracy: 0.9123\n",
            "Epoch 19/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.9183 - val_loss: 0.2399 - val_accuracy: 0.9123\n",
            "Epoch 20/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2625 - accuracy: 0.8966 - val_loss: 0.2346 - val_accuracy: 0.9123\n",
            "Epoch 21/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2241 - accuracy: 0.9094 - val_loss: 0.2343 - val_accuracy: 0.9123\n",
            "Epoch 22/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2623 - accuracy: 0.8863 - val_loss: 0.2458 - val_accuracy: 0.9035\n",
            "Epoch 23/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.9082 - val_loss: 0.2448 - val_accuracy: 0.9035\n",
            "Epoch 24/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1948 - accuracy: 0.9321 - val_loss: 0.2337 - val_accuracy: 0.9123\n",
            "Epoch 25/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1761 - accuracy: 0.9308 - val_loss: 0.2492 - val_accuracy: 0.8947\n",
            "Epoch 26/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2049 - accuracy: 0.9260 - val_loss: 0.2460 - val_accuracy: 0.9035\n",
            "Epoch 27/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2256 - accuracy: 0.9320 - val_loss: 0.2425 - val_accuracy: 0.9211\n",
            "Epoch 28/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1687 - accuracy: 0.9374 - val_loss: 0.2428 - val_accuracy: 0.9035\n",
            "Epoch 29/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2153 - accuracy: 0.9101 - val_loss: 0.2383 - val_accuracy: 0.9035\n",
            "Epoch 30/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2370 - accuracy: 0.9318 - val_loss: 0.2491 - val_accuracy: 0.9211\n",
            "Epoch 31/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1982 - accuracy: 0.9232 - val_loss: 0.2512 - val_accuracy: 0.8947\n",
            "Epoch 32/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2047 - accuracy: 0.9046 - val_loss: 0.2547 - val_accuracy: 0.9211\n",
            "Epoch 33/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2304 - accuracy: 0.9070 - val_loss: 0.2367 - val_accuracy: 0.9123\n",
            "Epoch 34/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1989 - accuracy: 0.9197 - val_loss: 0.2655 - val_accuracy: 0.8860\n",
            "Epoch 35/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2896 - accuracy: 0.9087 - val_loss: 0.2627 - val_accuracy: 0.9211\n",
            "Epoch 36/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1994 - accuracy: 0.9348 - val_loss: 0.2357 - val_accuracy: 0.9123\n",
            "Epoch 37/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2724 - accuracy: 0.9025 - val_loss: 0.2343 - val_accuracy: 0.9123\n",
            "Epoch 38/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1418 - accuracy: 0.9466 - val_loss: 0.3564 - val_accuracy: 0.8421\n",
            "Epoch 39/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.9044 - val_loss: 0.2610 - val_accuracy: 0.8860\n",
            "Epoch 40/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1821 - accuracy: 0.9247 - val_loss: 0.2606 - val_accuracy: 0.8860\n",
            "Epoch 41/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2208 - accuracy: 0.9134 - val_loss: 0.2332 - val_accuracy: 0.9123\n",
            "Epoch 42/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.9210 - val_loss: 0.2332 - val_accuracy: 0.9123\n",
            "Epoch 43/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2020 - accuracy: 0.9188 - val_loss: 0.3262 - val_accuracy: 0.9035\n",
            "Epoch 44/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2575 - accuracy: 0.9075 - val_loss: 0.2316 - val_accuracy: 0.9123\n",
            "Epoch 45/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1585 - accuracy: 0.9377 - val_loss: 0.2358 - val_accuracy: 0.9035\n",
            "Epoch 46/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1930 - accuracy: 0.9142 - val_loss: 0.2342 - val_accuracy: 0.9123\n",
            "Epoch 47/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1828 - accuracy: 0.9217 - val_loss: 0.2342 - val_accuracy: 0.9123\n",
            "Epoch 48/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2019 - accuracy: 0.9097 - val_loss: 0.5246 - val_accuracy: 0.7982\n",
            "Epoch 49/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2315 - accuracy: 0.9118 - val_loss: 0.2342 - val_accuracy: 0.9123\n",
            "Epoch 50/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1926 - accuracy: 0.9168 - val_loss: 0.2348 - val_accuracy: 0.9123\n",
            "Epoch 51/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1768 - accuracy: 0.9263 - val_loss: 0.2293 - val_accuracy: 0.9123\n",
            "Epoch 52/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1498 - accuracy: 0.9353 - val_loss: 0.2356 - val_accuracy: 0.9123\n",
            "Epoch 53/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1837 - accuracy: 0.9259 - val_loss: 0.2273 - val_accuracy: 0.9123\n",
            "Epoch 54/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1614 - accuracy: 0.9389 - val_loss: 0.3077 - val_accuracy: 0.8684\n",
            "Epoch 55/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1895 - accuracy: 0.9014 - val_loss: 0.3102 - val_accuracy: 0.8684\n",
            "Epoch 56/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1975 - accuracy: 0.9215 - val_loss: 0.2302 - val_accuracy: 0.9123\n",
            "Epoch 57/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.9256 - val_loss: 0.2439 - val_accuracy: 0.9211\n",
            "Epoch 58/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1957 - accuracy: 0.9259 - val_loss: 0.2299 - val_accuracy: 0.9123\n",
            "Epoch 59/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1913 - accuracy: 0.9168 - val_loss: 0.4150 - val_accuracy: 0.8596\n",
            "Epoch 60/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2049 - accuracy: 0.9197 - val_loss: 0.2273 - val_accuracy: 0.9123\n",
            "Epoch 61/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1728 - accuracy: 0.9355 - val_loss: 0.2263 - val_accuracy: 0.9123\n",
            "Epoch 62/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2234 - accuracy: 0.9214 - val_loss: 0.3268 - val_accuracy: 0.8684\n",
            "Epoch 63/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1958 - accuracy: 0.9195 - val_loss: 0.2430 - val_accuracy: 0.9123\n",
            "Epoch 64/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2402 - accuracy: 0.8854 - val_loss: 0.2667 - val_accuracy: 0.9211\n",
            "Epoch 65/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1640 - accuracy: 0.9452 - val_loss: 0.2285 - val_accuracy: 0.9123\n",
            "Epoch 66/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1609 - accuracy: 0.9179 - val_loss: 0.2293 - val_accuracy: 0.9123\n",
            "Epoch 67/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1904 - accuracy: 0.9343 - val_loss: 0.2283 - val_accuracy: 0.9123\n",
            "Epoch 68/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1648 - accuracy: 0.9362 - val_loss: 0.2596 - val_accuracy: 0.8772\n",
            "Epoch 69/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1923 - accuracy: 0.9207 - val_loss: 0.2254 - val_accuracy: 0.9123\n",
            "Epoch 70/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1375 - accuracy: 0.9542 - val_loss: 0.2246 - val_accuracy: 0.9123\n",
            "Epoch 71/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1565 - accuracy: 0.9401 - val_loss: 0.2292 - val_accuracy: 0.9123\n",
            "Epoch 72/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1549 - accuracy: 0.9337 - val_loss: 0.2331 - val_accuracy: 0.9035\n",
            "Epoch 73/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1832 - accuracy: 0.9300 - val_loss: 0.2233 - val_accuracy: 0.9123\n",
            "Epoch 74/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1557 - accuracy: 0.9364 - val_loss: 0.2223 - val_accuracy: 0.9123\n",
            "Epoch 75/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1358 - accuracy: 0.9410 - val_loss: 0.3672 - val_accuracy: 0.8509\n",
            "Epoch 76/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2536 - accuracy: 0.8989 - val_loss: 0.2584 - val_accuracy: 0.9211\n",
            "Epoch 77/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1583 - accuracy: 0.9384 - val_loss: 0.2227 - val_accuracy: 0.9123\n",
            "Epoch 78/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9100 - val_loss: 0.2361 - val_accuracy: 0.9123\n",
            "Epoch 79/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1473 - accuracy: 0.9317 - val_loss: 0.2759 - val_accuracy: 0.8772\n",
            "Epoch 80/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1880 - accuracy: 0.9263 - val_loss: 0.2753 - val_accuracy: 0.9211\n",
            "Epoch 81/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1913 - accuracy: 0.9249 - val_loss: 0.2278 - val_accuracy: 0.9211\n",
            "Epoch 82/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1477 - accuracy: 0.9527 - val_loss: 0.2311 - val_accuracy: 0.9035\n",
            "Epoch 83/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1478 - accuracy: 0.9459 - val_loss: 0.2431 - val_accuracy: 0.9211\n",
            "Epoch 84/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1665 - accuracy: 0.9146 - val_loss: 0.2327 - val_accuracy: 0.8947\n",
            "Epoch 85/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1437 - accuracy: 0.9489 - val_loss: 0.2288 - val_accuracy: 0.9211\n",
            "Epoch 86/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1578 - accuracy: 0.9388 - val_loss: 0.2520 - val_accuracy: 0.8772\n",
            "Epoch 87/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1701 - accuracy: 0.9254 - val_loss: 0.2203 - val_accuracy: 0.9123\n",
            "Epoch 88/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1143 - accuracy: 0.9523 - val_loss: 0.2544 - val_accuracy: 0.9211\n",
            "Epoch 89/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1541 - accuracy: 0.9395 - val_loss: 0.2301 - val_accuracy: 0.8947\n",
            "Epoch 90/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1747 - accuracy: 0.9326 - val_loss: 0.2303 - val_accuracy: 0.8947\n",
            "Epoch 91/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1761 - accuracy: 0.9292 - val_loss: 0.2243 - val_accuracy: 0.9211\n",
            "Epoch 92/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1170 - accuracy: 0.9579 - val_loss: 0.2190 - val_accuracy: 0.9123\n",
            "Epoch 93/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1722 - accuracy: 0.9269 - val_loss: 0.2343 - val_accuracy: 0.9123\n",
            "Epoch 94/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1492 - accuracy: 0.9394 - val_loss: 0.2356 - val_accuracy: 0.9211\n",
            "Epoch 95/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1965 - accuracy: 0.9233 - val_loss: 0.2471 - val_accuracy: 0.8772\n",
            "Epoch 96/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.9149 - val_loss: 0.2225 - val_accuracy: 0.9035\n",
            "Epoch 97/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1386 - accuracy: 0.9477 - val_loss: 0.2180 - val_accuracy: 0.9123\n",
            "Epoch 98/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1378 - accuracy: 0.9421 - val_loss: 0.2325 - val_accuracy: 0.8947\n",
            "Epoch 99/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1426 - accuracy: 0.9458 - val_loss: 0.2271 - val_accuracy: 0.9211\n",
            "Epoch 100/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1182 - accuracy: 0.9581 - val_loss: 0.2187 - val_accuracy: 0.9211\n",
            "Epoch 101/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1369 - accuracy: 0.9511 - val_loss: 0.2182 - val_accuracy: 0.9211\n",
            "Epoch 102/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1646 - accuracy: 0.9302 - val_loss: 0.2449 - val_accuracy: 0.8772\n",
            "Epoch 103/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1577 - accuracy: 0.9354 - val_loss: 0.2194 - val_accuracy: 0.9211\n",
            "Epoch 104/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1633 - accuracy: 0.9286 - val_loss: 0.2795 - val_accuracy: 0.8684\n",
            "Epoch 105/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1608 - accuracy: 0.9424 - val_loss: 0.2215 - val_accuracy: 0.9211\n",
            "Epoch 106/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1765 - accuracy: 0.9349 - val_loss: 0.2159 - val_accuracy: 0.9123\n",
            "Epoch 107/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1212 - accuracy: 0.9470 - val_loss: 0.2187 - val_accuracy: 0.9123\n",
            "Epoch 108/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1719 - accuracy: 0.9276 - val_loss: 0.2153 - val_accuracy: 0.9123\n",
            "Epoch 109/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1230 - accuracy: 0.9567 - val_loss: 0.2162 - val_accuracy: 0.9123\n",
            "Epoch 110/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1581 - accuracy: 0.9301 - val_loss: 0.2351 - val_accuracy: 0.8860\n",
            "Epoch 111/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1878 - accuracy: 0.9272 - val_loss: 0.2163 - val_accuracy: 0.9211\n",
            "Epoch 112/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1796 - accuracy: 0.9290 - val_loss: 0.2144 - val_accuracy: 0.9123\n",
            "Epoch 113/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1180 - accuracy: 0.9620 - val_loss: 0.2327 - val_accuracy: 0.9211\n",
            "Epoch 114/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2176 - accuracy: 0.9103 - val_loss: 0.2614 - val_accuracy: 0.8772\n",
            "Epoch 115/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1407 - accuracy: 0.9479 - val_loss: 0.2456 - val_accuracy: 0.9123\n",
            "Epoch 116/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1677 - accuracy: 0.9395 - val_loss: 0.2115 - val_accuracy: 0.9123\n",
            "Epoch 117/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1359 - accuracy: 0.9528 - val_loss: 0.2302 - val_accuracy: 0.9211\n",
            "Epoch 118/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1215 - accuracy: 0.9616 - val_loss: 0.2129 - val_accuracy: 0.9123\n",
            "Epoch 119/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1164 - accuracy: 0.9512 - val_loss: 0.2100 - val_accuracy: 0.9035\n",
            "Epoch 120/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1413 - accuracy: 0.9513 - val_loss: 0.2476 - val_accuracy: 0.9211\n",
            "Epoch 121/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1987 - accuracy: 0.9155 - val_loss: 0.2117 - val_accuracy: 0.9035\n",
            "Epoch 122/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1313 - accuracy: 0.9479 - val_loss: 0.2218 - val_accuracy: 0.9035\n",
            "Epoch 123/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1142 - accuracy: 0.9544 - val_loss: 0.2132 - val_accuracy: 0.9123\n",
            "Epoch 124/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1734 - accuracy: 0.9167 - val_loss: 0.2218 - val_accuracy: 0.9035\n",
            "Epoch 125/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1188 - accuracy: 0.9589 - val_loss: 0.2407 - val_accuracy: 0.8772\n",
            "Epoch 126/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1344 - accuracy: 0.9440 - val_loss: 0.2146 - val_accuracy: 0.9035\n",
            "Epoch 127/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1489 - accuracy: 0.9483 - val_loss: 0.2185 - val_accuracy: 0.9035\n",
            "Epoch 128/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1176 - accuracy: 0.9445 - val_loss: 0.2122 - val_accuracy: 0.9123\n",
            "Epoch 129/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1318 - accuracy: 0.9467 - val_loss: 0.2729 - val_accuracy: 0.8772\n",
            "Epoch 130/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1581 - accuracy: 0.9226 - val_loss: 0.2187 - val_accuracy: 0.9211\n",
            "Epoch 131/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2489 - accuracy: 0.8974 - val_loss: 0.2101 - val_accuracy: 0.9123\n",
            "Epoch 132/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1557 - accuracy: 0.9387 - val_loss: 0.2538 - val_accuracy: 0.8772\n",
            "Epoch 133/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1491 - accuracy: 0.9253 - val_loss: 0.2644 - val_accuracy: 0.9123\n",
            "Epoch 134/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1872 - accuracy: 0.9217 - val_loss: 0.2320 - val_accuracy: 0.8772\n",
            "Epoch 135/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1621 - accuracy: 0.9364 - val_loss: 0.2507 - val_accuracy: 0.9211\n",
            "Epoch 136/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1317 - accuracy: 0.9564 - val_loss: 0.2285 - val_accuracy: 0.8772\n",
            "Epoch 137/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.9189 - val_loss: 0.2122 - val_accuracy: 0.9035\n",
            "Epoch 138/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1557 - accuracy: 0.9293 - val_loss: 0.2287 - val_accuracy: 0.9123\n",
            "Epoch 139/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1289 - accuracy: 0.9443 - val_loss: 0.2207 - val_accuracy: 0.9123\n",
            "Epoch 140/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1063 - accuracy: 0.9644 - val_loss: 0.2288 - val_accuracy: 0.9035\n",
            "Epoch 141/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1288 - accuracy: 0.9361 - val_loss: 0.2148 - val_accuracy: 0.9035\n",
            "Epoch 142/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1108 - accuracy: 0.9534 - val_loss: 0.2257 - val_accuracy: 0.8772\n",
            "Epoch 143/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2228 - accuracy: 0.8989 - val_loss: 0.2300 - val_accuracy: 0.8772\n",
            "Epoch 144/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2030 - accuracy: 0.9160 - val_loss: 0.2228 - val_accuracy: 0.9298\n",
            "Epoch 145/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1594 - accuracy: 0.9398 - val_loss: 0.2740 - val_accuracy: 0.8772\n",
            "Epoch 146/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1749 - accuracy: 0.9251 - val_loss: 0.2635 - val_accuracy: 0.9211\n",
            "Epoch 147/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1505 - accuracy: 0.9425 - val_loss: 0.2315 - val_accuracy: 0.8772\n",
            "Epoch 148/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1714 - accuracy: 0.9394 - val_loss: 0.3288 - val_accuracy: 0.9123\n",
            "Epoch 149/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2043 - accuracy: 0.9115 - val_loss: 0.2213 - val_accuracy: 0.8772\n",
            "Epoch 150/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1253 - accuracy: 0.9484 - val_loss: 0.2104 - val_accuracy: 0.9035\n",
            "Epoch 151/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1225 - accuracy: 0.9591 - val_loss: 0.2288 - val_accuracy: 0.8772\n",
            "Epoch 152/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1554 - accuracy: 0.9403 - val_loss: 0.2728 - val_accuracy: 0.8772\n",
            "Epoch 153/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1562 - accuracy: 0.9417 - val_loss: 0.2196 - val_accuracy: 0.9211\n",
            "Epoch 154/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1035 - accuracy: 0.9663 - val_loss: 0.2415 - val_accuracy: 0.8860\n",
            "Epoch 155/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1492 - accuracy: 0.9342 - val_loss: 0.2194 - val_accuracy: 0.9035\n",
            "Epoch 156/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1355 - accuracy: 0.9410 - val_loss: 0.2377 - val_accuracy: 0.8860\n",
            "Epoch 157/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1369 - accuracy: 0.9387 - val_loss: 0.2546 - val_accuracy: 0.9298\n",
            "Epoch 158/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1350 - accuracy: 0.9433 - val_loss: 0.2285 - val_accuracy: 0.8772\n",
            "Epoch 159/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1195 - accuracy: 0.9562 - val_loss: 0.2065 - val_accuracy: 0.9035\n",
            "Epoch 160/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1459 - accuracy: 0.9367 - val_loss: 0.2698 - val_accuracy: 0.8772\n",
            "Epoch 161/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1405 - accuracy: 0.9426 - val_loss: 0.1998 - val_accuracy: 0.9123\n",
            "Epoch 162/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1106 - accuracy: 0.9553 - val_loss: 0.1997 - val_accuracy: 0.9123\n",
            "Epoch 163/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1141 - accuracy: 0.9503 - val_loss: 0.1983 - val_accuracy: 0.9035\n",
            "Epoch 164/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1484 - accuracy: 0.9340 - val_loss: 0.1972 - val_accuracy: 0.9035\n",
            "Epoch 165/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1516 - accuracy: 0.9285 - val_loss: 0.2198 - val_accuracy: 0.8772\n",
            "Epoch 166/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1222 - accuracy: 0.9381 - val_loss: 0.2182 - val_accuracy: 0.9211\n",
            "Epoch 167/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1402 - accuracy: 0.9443 - val_loss: 0.2083 - val_accuracy: 0.9035\n",
            "Epoch 168/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1602 - accuracy: 0.9248 - val_loss: 0.2005 - val_accuracy: 0.9211\n",
            "Epoch 169/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9227 - val_loss: 0.2314 - val_accuracy: 0.8947\n",
            "Epoch 170/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1501 - accuracy: 0.9388 - val_loss: 0.1958 - val_accuracy: 0.9035\n",
            "Epoch 171/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1112 - accuracy: 0.9602 - val_loss: 0.1964 - val_accuracy: 0.9123\n",
            "Epoch 172/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1272 - accuracy: 0.9473 - val_loss: 0.2192 - val_accuracy: 0.9211\n",
            "Epoch 173/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1051 - accuracy: 0.9563 - val_loss: 0.2322 - val_accuracy: 0.8947\n",
            "Epoch 174/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1369 - accuracy: 0.9559 - val_loss: 0.2180 - val_accuracy: 0.9211\n",
            "Epoch 175/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1179 - accuracy: 0.9617 - val_loss: 0.2062 - val_accuracy: 0.8947\n",
            "Epoch 176/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1145 - accuracy: 0.9554 - val_loss: 0.2210 - val_accuracy: 0.9123\n",
            "Epoch 177/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1135 - accuracy: 0.9534 - val_loss: 0.2540 - val_accuracy: 0.9211\n",
            "Epoch 178/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1726 - accuracy: 0.9219 - val_loss: 0.2157 - val_accuracy: 0.8772\n",
            "Epoch 179/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1095 - accuracy: 0.9569 - val_loss: 0.1945 - val_accuracy: 0.9123\n",
            "Epoch 180/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1151 - accuracy: 0.9478 - val_loss: 0.2308 - val_accuracy: 0.8947\n",
            "Epoch 181/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1461 - accuracy: 0.9405 - val_loss: 0.2453 - val_accuracy: 0.8860\n",
            "Epoch 182/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1583 - accuracy: 0.9402 - val_loss: 0.2305 - val_accuracy: 0.8947\n",
            "Epoch 183/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1277 - accuracy: 0.9452 - val_loss: 0.1982 - val_accuracy: 0.9035\n",
            "Epoch 184/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1326 - accuracy: 0.9444 - val_loss: 0.1943 - val_accuracy: 0.9123\n",
            "Epoch 185/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1168 - accuracy: 0.9472 - val_loss: 0.2093 - val_accuracy: 0.9298\n",
            "Epoch 186/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1409 - accuracy: 0.9366 - val_loss: 0.2040 - val_accuracy: 0.9123\n",
            "Epoch 187/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1255 - accuracy: 0.9523 - val_loss: 0.2010 - val_accuracy: 0.9123\n",
            "Epoch 188/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0883 - accuracy: 0.9671 - val_loss: 0.1946 - val_accuracy: 0.9123\n",
            "Epoch 189/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1154 - accuracy: 0.9456 - val_loss: 0.2009 - val_accuracy: 0.8947\n",
            "Epoch 190/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1793 - accuracy: 0.9218 - val_loss: 0.2340 - val_accuracy: 0.9211\n",
            "Epoch 191/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1309 - accuracy: 0.9420 - val_loss: 0.1999 - val_accuracy: 0.9298\n",
            "Epoch 192/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1456 - accuracy: 0.9327 - val_loss: 0.1899 - val_accuracy: 0.9123\n",
            "Epoch 193/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0973 - accuracy: 0.9665 - val_loss: 0.1926 - val_accuracy: 0.9035\n",
            "Epoch 194/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1163 - accuracy: 0.9540 - val_loss: 0.2872 - val_accuracy: 0.9211\n",
            "Epoch 195/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1306 - accuracy: 0.9426 - val_loss: 0.1961 - val_accuracy: 0.9035\n",
            "Epoch 196/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1447 - accuracy: 0.9338 - val_loss: 0.4308 - val_accuracy: 0.8421\n",
            "Epoch 197/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.2702 - accuracy: 0.9087 - val_loss: 0.1910 - val_accuracy: 0.9211\n",
            "Epoch 198/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1043 - accuracy: 0.9581 - val_loss: 0.2241 - val_accuracy: 0.8947\n",
            "Epoch 199/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1346 - accuracy: 0.9426 - val_loss: 0.1950 - val_accuracy: 0.9035\n",
            "Epoch 200/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1349 - accuracy: 0.9589 - val_loss: 0.1914 - val_accuracy: 0.9123\n",
            "Epoch 201/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1324 - accuracy: 0.9494 - val_loss: 0.2622 - val_accuracy: 0.8772\n",
            "Epoch 202/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1155 - accuracy: 0.9582 - val_loss: 0.2011 - val_accuracy: 0.8947\n",
            "Epoch 203/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1135 - accuracy: 0.9413 - val_loss: 0.1989 - val_accuracy: 0.9298\n",
            "Epoch 204/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0971 - accuracy: 0.9687 - val_loss: 0.1933 - val_accuracy: 0.9035\n",
            "Epoch 205/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0976 - accuracy: 0.9637 - val_loss: 0.1909 - val_accuracy: 0.9123\n",
            "Epoch 206/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1163 - accuracy: 0.9533 - val_loss: 0.1888 - val_accuracy: 0.9123\n",
            "Epoch 207/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0934 - accuracy: 0.9659 - val_loss: 0.1903 - val_accuracy: 0.9035\n",
            "Epoch 208/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1143 - accuracy: 0.9467 - val_loss: 0.1871 - val_accuracy: 0.9123\n",
            "Epoch 209/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1290 - accuracy: 0.9436 - val_loss: 0.1865 - val_accuracy: 0.9123\n",
            "Epoch 210/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1105 - accuracy: 0.9542 - val_loss: 0.2464 - val_accuracy: 0.8772\n",
            "Epoch 211/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1372 - accuracy: 0.9380 - val_loss: 0.2066 - val_accuracy: 0.8947\n",
            "Epoch 212/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1083 - accuracy: 0.9549 - val_loss: 0.2056 - val_accuracy: 0.9035\n",
            "Epoch 213/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.9738 - val_loss: 0.1987 - val_accuracy: 0.8947\n",
            "Epoch 214/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1110 - accuracy: 0.9571 - val_loss: 0.1950 - val_accuracy: 0.9035\n",
            "Epoch 215/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1214 - accuracy: 0.9472 - val_loss: 0.1887 - val_accuracy: 0.9035\n",
            "Epoch 216/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1347 - accuracy: 0.9302 - val_loss: 0.1890 - val_accuracy: 0.9035\n",
            "Epoch 217/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1071 - accuracy: 0.9568 - val_loss: 0.1979 - val_accuracy: 0.9035\n",
            "Epoch 218/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1259 - accuracy: 0.9500 - val_loss: 0.1884 - val_accuracy: 0.9211\n",
            "Epoch 219/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1132 - accuracy: 0.9556 - val_loss: 0.3473 - val_accuracy: 0.9211\n",
            "Epoch 220/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9387 - val_loss: 0.1894 - val_accuracy: 0.9035\n",
            "Epoch 221/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1063 - accuracy: 0.9485 - val_loss: 0.1954 - val_accuracy: 0.9298\n",
            "Epoch 222/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1181 - accuracy: 0.9484 - val_loss: 0.1885 - val_accuracy: 0.9035\n",
            "Epoch 223/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1022 - accuracy: 0.9578 - val_loss: 0.1870 - val_accuracy: 0.9035\n",
            "Epoch 224/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1109 - accuracy: 0.9607 - val_loss: 0.1870 - val_accuracy: 0.9035\n",
            "Epoch 225/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1238 - accuracy: 0.9330 - val_loss: 0.2100 - val_accuracy: 0.9035\n",
            "Epoch 226/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1174 - accuracy: 0.9509 - val_loss: 0.1827 - val_accuracy: 0.9123\n",
            "Epoch 227/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 0.9565 - val_loss: 0.2062 - val_accuracy: 0.9298\n",
            "Epoch 228/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1001 - accuracy: 0.9607 - val_loss: 0.2696 - val_accuracy: 0.8860\n",
            "Epoch 229/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1396 - accuracy: 0.9390 - val_loss: 0.1809 - val_accuracy: 0.9211\n",
            "Epoch 230/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1013 - accuracy: 0.9580 - val_loss: 0.2066 - val_accuracy: 0.9035\n",
            "Epoch 231/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1351 - accuracy: 0.9306 - val_loss: 0.1842 - val_accuracy: 0.9035\n",
            "Epoch 232/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0823 - accuracy: 0.9640 - val_loss: 0.1810 - val_accuracy: 0.9211\n",
            "Epoch 233/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1302 - accuracy: 0.9440 - val_loss: 0.1884 - val_accuracy: 0.9211\n",
            "Epoch 234/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1343 - accuracy: 0.9450 - val_loss: 0.1832 - val_accuracy: 0.9035\n",
            "Epoch 235/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1025 - accuracy: 0.9605 - val_loss: 0.2060 - val_accuracy: 0.9211\n",
            "Epoch 236/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1007 - accuracy: 0.9565 - val_loss: 0.2387 - val_accuracy: 0.8772\n",
            "Epoch 237/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1228 - accuracy: 0.9609 - val_loss: 0.2395 - val_accuracy: 0.9211\n",
            "Epoch 238/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1072 - accuracy: 0.9422 - val_loss: 0.2374 - val_accuracy: 0.8772\n",
            "Epoch 239/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1166 - accuracy: 0.9470 - val_loss: 0.1908 - val_accuracy: 0.9035\n",
            "Epoch 240/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1065 - accuracy: 0.9541 - val_loss: 0.1864 - val_accuracy: 0.9211\n",
            "Epoch 241/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.9737 - val_loss: 0.1818 - val_accuracy: 0.9211\n",
            "Epoch 242/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1241 - accuracy: 0.9459 - val_loss: 0.2237 - val_accuracy: 0.9211\n",
            "Epoch 243/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1181 - accuracy: 0.9511 - val_loss: 0.1879 - val_accuracy: 0.9211\n",
            "Epoch 244/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1053 - accuracy: 0.9684 - val_loss: 0.1881 - val_accuracy: 0.9035\n",
            "Epoch 245/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1083 - accuracy: 0.9509 - val_loss: 0.1814 - val_accuracy: 0.9123\n",
            "Epoch 246/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1169 - accuracy: 0.9582 - val_loss: 0.1826 - val_accuracy: 0.9211\n",
            "Epoch 247/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1241 - accuracy: 0.9525 - val_loss: 0.2177 - val_accuracy: 0.8947\n",
            "Epoch 248/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1123 - accuracy: 0.9544 - val_loss: 0.2112 - val_accuracy: 0.9035\n",
            "Epoch 249/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0983 - accuracy: 0.9536 - val_loss: 0.1903 - val_accuracy: 0.9035\n",
            "Epoch 250/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1109 - accuracy: 0.9529 - val_loss: 0.1811 - val_accuracy: 0.9035\n",
            "Epoch 251/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1104 - accuracy: 0.9505 - val_loss: 0.1795 - val_accuracy: 0.9211\n",
            "Epoch 252/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0977 - accuracy: 0.9601 - val_loss: 0.1864 - val_accuracy: 0.9211\n",
            "Epoch 253/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1055 - accuracy: 0.9565 - val_loss: 0.2206 - val_accuracy: 0.9298\n",
            "Epoch 254/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.9652 - val_loss: 0.1817 - val_accuracy: 0.9035\n",
            "Epoch 255/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.9632 - val_loss: 0.2486 - val_accuracy: 0.9211\n",
            "Epoch 256/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1604 - accuracy: 0.9325 - val_loss: 0.3288 - val_accuracy: 0.9123\n",
            "Epoch 257/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1677 - accuracy: 0.9351 - val_loss: 0.1867 - val_accuracy: 0.9035\n",
            "Epoch 258/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1033 - accuracy: 0.9599 - val_loss: 0.1768 - val_accuracy: 0.9123\n",
            "Epoch 259/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1021 - accuracy: 0.9578 - val_loss: 0.1724 - val_accuracy: 0.9123\n",
            "Epoch 260/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0995 - accuracy: 0.9444 - val_loss: 0.1742 - val_accuracy: 0.9123\n",
            "Epoch 261/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0950 - accuracy: 0.9720 - val_loss: 0.1860 - val_accuracy: 0.9035\n",
            "Epoch 262/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0866 - accuracy: 0.9740 - val_loss: 0.1762 - val_accuracy: 0.9123\n",
            "Epoch 263/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0943 - accuracy: 0.9522 - val_loss: 0.1869 - val_accuracy: 0.9211\n",
            "Epoch 264/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1171 - accuracy: 0.9358 - val_loss: 0.1811 - val_accuracy: 0.9035\n",
            "Epoch 265/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1868 - accuracy: 0.9317 - val_loss: 0.1839 - val_accuracy: 0.9035\n",
            "Epoch 266/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 0.9582 - val_loss: 0.1746 - val_accuracy: 0.9123\n",
            "Epoch 267/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0924 - accuracy: 0.9679 - val_loss: 0.1874 - val_accuracy: 0.9298\n",
            "Epoch 268/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1035 - accuracy: 0.9488 - val_loss: 0.1739 - val_accuracy: 0.9123\n",
            "Epoch 269/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1097 - accuracy: 0.9620 - val_loss: 0.2576 - val_accuracy: 0.9211\n",
            "Epoch 270/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1663 - accuracy: 0.9244 - val_loss: 0.1736 - val_accuracy: 0.9211\n",
            "Epoch 271/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1302 - accuracy: 0.9456 - val_loss: 0.1742 - val_accuracy: 0.9211\n",
            "Epoch 272/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1299 - accuracy: 0.9335 - val_loss: 0.1762 - val_accuracy: 0.9123\n",
            "Epoch 273/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1348 - accuracy: 0.9412 - val_loss: 0.1752 - val_accuracy: 0.9123\n",
            "Epoch 274/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1207 - accuracy: 0.9390 - val_loss: 0.1904 - val_accuracy: 0.9035\n",
            "Epoch 275/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1159 - accuracy: 0.9534 - val_loss: 0.2134 - val_accuracy: 0.9211\n",
            "Epoch 276/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1201 - accuracy: 0.9640 - val_loss: 0.2083 - val_accuracy: 0.9035\n",
            "Epoch 277/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.9486 - val_loss: 0.1767 - val_accuracy: 0.9123\n",
            "Epoch 278/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1138 - accuracy: 0.9501 - val_loss: 0.1814 - val_accuracy: 0.9035\n",
            "Epoch 279/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0939 - accuracy: 0.9712 - val_loss: 0.1734 - val_accuracy: 0.9211\n",
            "Epoch 280/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0933 - accuracy: 0.9624 - val_loss: 0.1738 - val_accuracy: 0.9298\n",
            "Epoch 281/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0851 - accuracy: 0.9538 - val_loss: 0.1738 - val_accuracy: 0.9211\n",
            "Epoch 282/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1187 - accuracy: 0.9391 - val_loss: 0.1866 - val_accuracy: 0.9035\n",
            "Epoch 283/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1010 - accuracy: 0.9493 - val_loss: 0.1803 - val_accuracy: 0.9035\n",
            "Epoch 284/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1136 - accuracy: 0.9410 - val_loss: 0.1718 - val_accuracy: 0.9123\n",
            "Epoch 285/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1294 - accuracy: 0.9510 - val_loss: 0.2258 - val_accuracy: 0.8860\n",
            "Epoch 286/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1247 - accuracy: 0.9517 - val_loss: 0.1696 - val_accuracy: 0.9211\n",
            "Epoch 287/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0800 - accuracy: 0.9666 - val_loss: 0.1707 - val_accuracy: 0.9211\n",
            "Epoch 288/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1328 - accuracy: 0.9432 - val_loss: 0.1937 - val_accuracy: 0.9123\n",
            "Epoch 289/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1356 - accuracy: 0.9571 - val_loss: 0.2593 - val_accuracy: 0.9298\n",
            "Epoch 290/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1405 - accuracy: 0.9440 - val_loss: 0.1760 - val_accuracy: 0.9035\n",
            "Epoch 291/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0924 - accuracy: 0.9734 - val_loss: 0.2384 - val_accuracy: 0.8772\n",
            "Epoch 292/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1240 - accuracy: 0.9483 - val_loss: 0.1718 - val_accuracy: 0.9298\n",
            "Epoch 293/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1029 - accuracy: 0.9510 - val_loss: 0.1974 - val_accuracy: 0.9035\n",
            "Epoch 294/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0943 - accuracy: 0.9499 - val_loss: 0.1725 - val_accuracy: 0.9211\n",
            "Epoch 295/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1160 - accuracy: 0.9554 - val_loss: 0.1781 - val_accuracy: 0.9035\n",
            "Epoch 296/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0818 - accuracy: 0.9684 - val_loss: 0.2085 - val_accuracy: 0.9298\n",
            "Epoch 297/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1091 - accuracy: 0.9471 - val_loss: 0.1744 - val_accuracy: 0.9298\n",
            "Epoch 298/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.9689 - val_loss: 0.2370 - val_accuracy: 0.9298\n",
            "Epoch 299/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1298 - accuracy: 0.9368 - val_loss: 0.2100 - val_accuracy: 0.9035\n",
            "Epoch 300/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1022 - accuracy: 0.9569 - val_loss: 0.1792 - val_accuracy: 0.9298\n",
            "Epoch 301/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1136 - accuracy: 0.9506 - val_loss: 0.1909 - val_accuracy: 0.9035\n",
            "Epoch 302/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1010 - accuracy: 0.9557 - val_loss: 0.1976 - val_accuracy: 0.9123\n",
            "Epoch 303/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1099 - accuracy: 0.9611 - val_loss: 0.1951 - val_accuracy: 0.9035\n",
            "Epoch 304/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1100 - accuracy: 0.9432 - val_loss: 0.1774 - val_accuracy: 0.9035\n",
            "Epoch 305/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0920 - accuracy: 0.9699 - val_loss: 0.1729 - val_accuracy: 0.9298\n",
            "Epoch 306/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1204 - accuracy: 0.9355 - val_loss: 0.1710 - val_accuracy: 0.9298\n",
            "Epoch 307/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1016 - accuracy: 0.9646 - val_loss: 0.1708 - val_accuracy: 0.9211\n",
            "Epoch 308/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1101 - accuracy: 0.9554 - val_loss: 0.1981 - val_accuracy: 0.9123\n",
            "Epoch 309/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0983 - accuracy: 0.9665 - val_loss: 0.1944 - val_accuracy: 0.9298\n",
            "Epoch 310/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1084 - accuracy: 0.9528 - val_loss: 0.2701 - val_accuracy: 0.9035\n",
            "Epoch 311/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1048 - accuracy: 0.9747 - val_loss: 0.1823 - val_accuracy: 0.9035\n",
            "Epoch 312/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1475 - accuracy: 0.9475 - val_loss: 0.1677 - val_accuracy: 0.9211\n",
            "Epoch 313/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1361 - accuracy: 0.9430 - val_loss: 0.1791 - val_accuracy: 0.9298\n",
            "Epoch 314/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1411 - accuracy: 0.9363 - val_loss: 0.1731 - val_accuracy: 0.9035\n",
            "Epoch 315/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1242 - accuracy: 0.9414 - val_loss: 0.1882 - val_accuracy: 0.9035\n",
            "Epoch 316/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1000 - accuracy: 0.9634 - val_loss: 0.1748 - val_accuracy: 0.9035\n",
            "Epoch 317/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0922 - accuracy: 0.9592 - val_loss: 0.1823 - val_accuracy: 0.9035\n",
            "Epoch 318/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1034 - accuracy: 0.9478 - val_loss: 0.1818 - val_accuracy: 0.9298\n",
            "Epoch 319/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0869 - accuracy: 0.9658 - val_loss: 0.1829 - val_accuracy: 0.9298\n",
            "Epoch 320/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2039 - accuracy: 0.9240 - val_loss: 0.1774 - val_accuracy: 0.9298\n",
            "Epoch 321/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.9544 - val_loss: 0.1651 - val_accuracy: 0.9298\n",
            "Epoch 322/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1071 - accuracy: 0.9595 - val_loss: 0.1681 - val_accuracy: 0.9298\n",
            "Epoch 323/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1078 - accuracy: 0.9648 - val_loss: 0.2116 - val_accuracy: 0.9035\n",
            "Epoch 324/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9327 - val_loss: 0.1861 - val_accuracy: 0.9298\n",
            "Epoch 325/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1278 - accuracy: 0.9599 - val_loss: 0.1691 - val_accuracy: 0.9211\n",
            "Epoch 326/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0893 - accuracy: 0.9554 - val_loss: 0.1709 - val_accuracy: 0.9035\n",
            "Epoch 327/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1067 - accuracy: 0.9660 - val_loss: 0.2544 - val_accuracy: 0.9298\n",
            "Epoch 328/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1903 - accuracy: 0.9362 - val_loss: 0.2906 - val_accuracy: 0.9035\n",
            "Epoch 329/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1329 - accuracy: 0.9314 - val_loss: 0.1781 - val_accuracy: 0.9035\n",
            "Epoch 330/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0886 - accuracy: 0.9706 - val_loss: 0.2204 - val_accuracy: 0.9035\n",
            "Epoch 331/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1635 - accuracy: 0.9291 - val_loss: 0.1786 - val_accuracy: 0.9035\n",
            "Epoch 332/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1105 - accuracy: 0.9521 - val_loss: 0.2237 - val_accuracy: 0.8947\n",
            "Epoch 333/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0925 - accuracy: 0.9654 - val_loss: 0.1945 - val_accuracy: 0.9298\n",
            "Epoch 334/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1470 - accuracy: 0.9544 - val_loss: 0.1737 - val_accuracy: 0.9035\n",
            "Epoch 335/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1100 - accuracy: 0.9492 - val_loss: 0.1704 - val_accuracy: 0.9298\n",
            "Epoch 336/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0626 - accuracy: 0.9723 - val_loss: 0.1753 - val_accuracy: 0.9298\n",
            "Epoch 337/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0886 - accuracy: 0.9592 - val_loss: 0.2396 - val_accuracy: 0.8860\n",
            "Epoch 338/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0811 - accuracy: 0.9739 - val_loss: 0.1794 - val_accuracy: 0.9035\n",
            "Epoch 339/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1221 - accuracy: 0.9578 - val_loss: 0.2125 - val_accuracy: 0.9298\n",
            "Epoch 340/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.9667 - val_loss: 0.1733 - val_accuracy: 0.9298\n",
            "Epoch 341/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0917 - accuracy: 0.9647 - val_loss: 0.1757 - val_accuracy: 0.9211\n",
            "Epoch 342/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1163 - accuracy: 0.9489 - val_loss: 0.1880 - val_accuracy: 0.9035\n",
            "Epoch 343/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.1142 - accuracy: 0.9631 - val_loss: 0.2273 - val_accuracy: 0.9298\n",
            "Epoch 344/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1279 - accuracy: 0.9441 - val_loss: 0.2385 - val_accuracy: 0.9211\n",
            "Epoch 345/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.1670 - accuracy: 0.9143 - val_loss: 0.1705 - val_accuracy: 0.9211\n",
            "Epoch 346/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1021 - accuracy: 0.9638 - val_loss: 0.1933 - val_accuracy: 0.9123\n",
            "Epoch 347/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1047 - accuracy: 0.9462 - val_loss: 0.1779 - val_accuracy: 0.9035\n",
            "Epoch 348/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.9650 - val_loss: 0.1861 - val_accuracy: 0.9298\n",
            "Epoch 349/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0916 - accuracy: 0.9741 - val_loss: 0.1870 - val_accuracy: 0.9035\n",
            "Epoch 350/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1163 - accuracy: 0.9646 - val_loss: 0.1820 - val_accuracy: 0.9386\n",
            "Epoch 351/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1305 - accuracy: 0.9484 - val_loss: 0.1733 - val_accuracy: 0.9035\n",
            "Epoch 352/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1028 - accuracy: 0.9521 - val_loss: 0.1931 - val_accuracy: 0.9123\n",
            "Epoch 353/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9768 - val_loss: 0.1674 - val_accuracy: 0.9386\n",
            "Epoch 354/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1240 - accuracy: 0.9470 - val_loss: 0.2415 - val_accuracy: 0.9298\n",
            "Epoch 355/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.1290 - accuracy: 0.9448 - val_loss: 0.1704 - val_accuracy: 0.9035\n",
            "Epoch 356/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1080 - accuracy: 0.9555 - val_loss: 0.1716 - val_accuracy: 0.9035\n",
            "Epoch 357/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0881 - accuracy: 0.9635 - val_loss: 0.1677 - val_accuracy: 0.9386\n",
            "Epoch 358/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1101 - accuracy: 0.9647 - val_loss: 0.1873 - val_accuracy: 0.9035\n",
            "Epoch 359/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1330 - accuracy: 0.9471 - val_loss: 0.1919 - val_accuracy: 0.9123\n",
            "Epoch 360/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1216 - accuracy: 0.9449 - val_loss: 0.1631 - val_accuracy: 0.9211\n",
            "Epoch 361/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0864 - accuracy: 0.9703 - val_loss: 0.1658 - val_accuracy: 0.9298\n",
            "Epoch 362/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0640 - accuracy: 0.9788 - val_loss: 0.1643 - val_accuracy: 0.9386\n",
            "Epoch 363/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9526 - val_loss: 0.1635 - val_accuracy: 0.9211\n",
            "Epoch 364/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0799 - accuracy: 0.9645 - val_loss: 0.1653 - val_accuracy: 0.9123\n",
            "Epoch 365/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1041 - accuracy: 0.9614 - val_loss: 0.1832 - val_accuracy: 0.9035\n",
            "Epoch 366/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1076 - accuracy: 0.9568 - val_loss: 0.1870 - val_accuracy: 0.9123\n",
            "Epoch 367/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0882 - accuracy: 0.9601 - val_loss: 0.2107 - val_accuracy: 0.9298\n",
            "Epoch 368/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0722 - accuracy: 0.9711 - val_loss: 0.1763 - val_accuracy: 0.9035\n",
            "Epoch 369/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.9633 - val_loss: 0.1658 - val_accuracy: 0.9211\n",
            "Epoch 370/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1382 - accuracy: 0.9502 - val_loss: 0.2095 - val_accuracy: 0.9123\n",
            "Epoch 371/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1134 - accuracy: 0.9592 - val_loss: 0.1776 - val_accuracy: 0.9298\n",
            "Epoch 372/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1356 - accuracy: 0.9405 - val_loss: 0.1759 - val_accuracy: 0.9035\n",
            "Epoch 373/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1064 - accuracy: 0.9574 - val_loss: 0.1648 - val_accuracy: 0.9298\n",
            "Epoch 374/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0820 - accuracy: 0.9656 - val_loss: 0.1923 - val_accuracy: 0.9123\n",
            "Epoch 375/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.1083 - accuracy: 0.9708 - val_loss: 0.1888 - val_accuracy: 0.9123\n",
            "Epoch 376/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0787 - accuracy: 0.9694 - val_loss: 0.1650 - val_accuracy: 0.9386\n",
            "Epoch 377/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0838 - accuracy: 0.9627 - val_loss: 0.2753 - val_accuracy: 0.9035\n",
            "Epoch 378/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.1961 - accuracy: 0.9094 - val_loss: 0.2061 - val_accuracy: 0.9123\n",
            "Epoch 379/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.9605 - val_loss: 0.1681 - val_accuracy: 0.9386\n",
            "Epoch 380/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1046 - accuracy: 0.9444 - val_loss: 0.1702 - val_accuracy: 0.9386\n",
            "Epoch 381/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0940 - accuracy: 0.9580 - val_loss: 0.1716 - val_accuracy: 0.9386\n",
            "Epoch 382/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.9648 - val_loss: 0.1736 - val_accuracy: 0.9035\n",
            "Epoch 383/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0932 - accuracy: 0.9735 - val_loss: 0.1735 - val_accuracy: 0.9386\n",
            "Epoch 384/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0700 - accuracy: 0.9646 - val_loss: 0.1706 - val_accuracy: 0.9386\n",
            "Epoch 385/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1068 - accuracy: 0.9564 - val_loss: 0.1712 - val_accuracy: 0.9035\n",
            "Epoch 386/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1083 - accuracy: 0.9689 - val_loss: 0.2044 - val_accuracy: 0.9123\n",
            "Epoch 387/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1321 - accuracy: 0.9412 - val_loss: 0.2095 - val_accuracy: 0.9123\n",
            "Epoch 388/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0879 - accuracy: 0.9626 - val_loss: 0.1704 - val_accuracy: 0.9123\n",
            "Epoch 389/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1083 - accuracy: 0.9577 - val_loss: 0.1795 - val_accuracy: 0.9035\n",
            "Epoch 390/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0803 - accuracy: 0.9663 - val_loss: 0.1860 - val_accuracy: 0.9035\n",
            "Epoch 391/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1297 - accuracy: 0.9513 - val_loss: 0.1737 - val_accuracy: 0.9035\n",
            "Epoch 392/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1094 - accuracy: 0.9621 - val_loss: 0.2422 - val_accuracy: 0.9298\n",
            "Epoch 393/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1203 - accuracy: 0.9446 - val_loss: 0.1958 - val_accuracy: 0.9298\n",
            "Epoch 394/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0862 - accuracy: 0.9571 - val_loss: 0.3886 - val_accuracy: 0.8947\n",
            "Epoch 395/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1293 - accuracy: 0.9344 - val_loss: 0.1713 - val_accuracy: 0.9211\n",
            "Epoch 396/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0835 - accuracy: 0.9699 - val_loss: 0.1751 - val_accuracy: 0.9386\n",
            "Epoch 397/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1560 - accuracy: 0.9400 - val_loss: 0.1708 - val_accuracy: 0.9386\n",
            "Epoch 398/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1744 - accuracy: 0.9338 - val_loss: 0.2033 - val_accuracy: 0.9123\n",
            "Epoch 399/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1481 - accuracy: 0.9417 - val_loss: 0.1726 - val_accuracy: 0.9298\n",
            "Epoch 400/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9670 - val_loss: 0.1877 - val_accuracy: 0.9123\n",
            "Epoch 401/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1019 - accuracy: 0.9534 - val_loss: 0.1698 - val_accuracy: 0.9035\n",
            "Epoch 402/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.9651 - val_loss: 0.1676 - val_accuracy: 0.9211\n",
            "Epoch 403/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1020 - accuracy: 0.9678 - val_loss: 0.1692 - val_accuracy: 0.9123\n",
            "Epoch 404/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1084 - accuracy: 0.9537 - val_loss: 0.2049 - val_accuracy: 0.9123\n",
            "Epoch 405/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1333 - accuracy: 0.9439 - val_loss: 0.1765 - val_accuracy: 0.9386\n",
            "Epoch 406/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1309 - accuracy: 0.9617 - val_loss: 0.1741 - val_accuracy: 0.9035\n",
            "Epoch 407/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0904 - accuracy: 0.9556 - val_loss: 0.1682 - val_accuracy: 0.9386\n",
            "Epoch 408/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0948 - accuracy: 0.9682 - val_loss: 0.1803 - val_accuracy: 0.9386\n",
            "Epoch 409/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0926 - accuracy: 0.9656 - val_loss: 0.3105 - val_accuracy: 0.9035\n",
            "Epoch 410/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1799 - accuracy: 0.9282 - val_loss: 0.1845 - val_accuracy: 0.9123\n",
            "Epoch 411/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0959 - accuracy: 0.9693 - val_loss: 0.2471 - val_accuracy: 0.9035\n",
            "Epoch 412/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0843 - accuracy: 0.9735 - val_loss: 0.1913 - val_accuracy: 0.9123\n",
            "Epoch 413/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0974 - accuracy: 0.9618 - val_loss: 0.1658 - val_accuracy: 0.9386\n",
            "Epoch 414/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0992 - accuracy: 0.9606 - val_loss: 0.1770 - val_accuracy: 0.9035\n",
            "Epoch 415/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0879 - accuracy: 0.9694 - val_loss: 0.2056 - val_accuracy: 0.9386\n",
            "Epoch 416/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1054 - accuracy: 0.9584 - val_loss: 0.1939 - val_accuracy: 0.9123\n",
            "Epoch 417/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0849 - accuracy: 0.9692 - val_loss: 0.1630 - val_accuracy: 0.9386\n",
            "Epoch 418/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0950 - accuracy: 0.9525 - val_loss: 0.1654 - val_accuracy: 0.9035\n",
            "Epoch 419/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0994 - accuracy: 0.9566 - val_loss: 0.2050 - val_accuracy: 0.9123\n",
            "Epoch 420/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0817 - accuracy: 0.9685 - val_loss: 0.1711 - val_accuracy: 0.9035\n",
            "Epoch 421/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0836 - accuracy: 0.9654 - val_loss: 0.1656 - val_accuracy: 0.9386\n",
            "Epoch 422/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0843 - accuracy: 0.9723 - val_loss: 0.1769 - val_accuracy: 0.9035\n",
            "Epoch 423/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0933 - accuracy: 0.9611 - val_loss: 0.1743 - val_accuracy: 0.9386\n",
            "Epoch 424/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.1318 - accuracy: 0.9598 - val_loss: 0.1790 - val_accuracy: 0.9386\n",
            "Epoch 425/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0983 - accuracy: 0.9547 - val_loss: 0.1688 - val_accuracy: 0.9035\n",
            "Epoch 426/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0770 - accuracy: 0.9748 - val_loss: 0.1809 - val_accuracy: 0.9386\n",
            "Epoch 427/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.9719 - val_loss: 0.1639 - val_accuracy: 0.9035\n",
            "Epoch 428/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0968 - accuracy: 0.9596 - val_loss: 0.1632 - val_accuracy: 0.9386\n",
            "Epoch 429/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1142 - accuracy: 0.9441 - val_loss: 0.1656 - val_accuracy: 0.9035\n",
            "Epoch 430/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0886 - accuracy: 0.9625 - val_loss: 0.1706 - val_accuracy: 0.9035\n",
            "Epoch 431/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1088 - accuracy: 0.9687 - val_loss: 0.1751 - val_accuracy: 0.9386\n",
            "Epoch 432/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1083 - accuracy: 0.9680 - val_loss: 0.2107 - val_accuracy: 0.9035\n",
            "Epoch 433/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1013 - accuracy: 0.9470 - val_loss: 0.1813 - val_accuracy: 0.9386\n",
            "Epoch 434/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0872 - accuracy: 0.9697 - val_loss: 0.3483 - val_accuracy: 0.9035\n",
            "Epoch 435/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1293 - accuracy: 0.9561 - val_loss: 0.1724 - val_accuracy: 0.9298\n",
            "Epoch 436/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1691 - accuracy: 0.9403 - val_loss: 0.1979 - val_accuracy: 0.9123\n",
            "Epoch 437/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1244 - accuracy: 0.9515 - val_loss: 0.1616 - val_accuracy: 0.9386\n",
            "Epoch 438/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0962 - accuracy: 0.9606 - val_loss: 0.1979 - val_accuracy: 0.9386\n",
            "Epoch 439/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1016 - accuracy: 0.9650 - val_loss: 0.1641 - val_accuracy: 0.9386\n",
            "Epoch 440/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0738 - accuracy: 0.9643 - val_loss: 0.1982 - val_accuracy: 0.9123\n",
            "Epoch 441/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1199 - accuracy: 0.9566 - val_loss: 0.1640 - val_accuracy: 0.9035\n",
            "Epoch 442/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1211 - accuracy: 0.9541 - val_loss: 0.1675 - val_accuracy: 0.9035\n",
            "Epoch 443/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 0.9755 - val_loss: 0.1672 - val_accuracy: 0.9035\n",
            "Epoch 444/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0629 - accuracy: 0.9790 - val_loss: 0.1912 - val_accuracy: 0.9386\n",
            "Epoch 445/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.1614 - accuracy: 0.9511 - val_loss: 0.1927 - val_accuracy: 0.9123\n",
            "Epoch 446/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1013 - accuracy: 0.9552 - val_loss: 0.1625 - val_accuracy: 0.9386\n",
            "Epoch 447/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1016 - accuracy: 0.9588 - val_loss: 0.2007 - val_accuracy: 0.9386\n",
            "Epoch 448/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0992 - accuracy: 0.9622 - val_loss: 0.1641 - val_accuracy: 0.9386\n",
            "Epoch 449/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0804 - accuracy: 0.9649 - val_loss: 0.1722 - val_accuracy: 0.9035\n",
            "Epoch 450/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1063 - accuracy: 0.9564 - val_loss: 0.1711 - val_accuracy: 0.9035\n",
            "Epoch 451/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.9708 - val_loss: 0.1628 - val_accuracy: 0.9386\n",
            "Epoch 452/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0889 - accuracy: 0.9627 - val_loss: 0.2105 - val_accuracy: 0.8947\n",
            "Epoch 453/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1013 - accuracy: 0.9577 - val_loss: 0.1919 - val_accuracy: 0.9123\n",
            "Epoch 454/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0917 - accuracy: 0.9666 - val_loss: 0.1843 - val_accuracy: 0.9386\n",
            "Epoch 455/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0858 - accuracy: 0.9587 - val_loss: 0.1632 - val_accuracy: 0.9386\n",
            "Epoch 456/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0964 - accuracy: 0.9660 - val_loss: 0.1905 - val_accuracy: 0.9123\n",
            "Epoch 457/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1293 - accuracy: 0.9431 - val_loss: 0.1858 - val_accuracy: 0.9123\n",
            "Epoch 458/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0871 - accuracy: 0.9559 - val_loss: 0.2191 - val_accuracy: 0.9123\n",
            "Epoch 459/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0925 - accuracy: 0.9692 - val_loss: 0.2027 - val_accuracy: 0.9386\n",
            "Epoch 460/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0820 - accuracy: 0.9663 - val_loss: 0.1868 - val_accuracy: 0.9386\n",
            "Epoch 461/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0781 - accuracy: 0.9742 - val_loss: 0.2046 - val_accuracy: 0.9123\n",
            "Epoch 462/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1402 - accuracy: 0.9674 - val_loss: 0.1686 - val_accuracy: 0.9123\n",
            "Epoch 463/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0879 - accuracy: 0.9660 - val_loss: 0.2140 - val_accuracy: 0.9123\n",
            "Epoch 464/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1036 - accuracy: 0.9607 - val_loss: 0.1718 - val_accuracy: 0.9035\n",
            "Epoch 465/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0881 - accuracy: 0.9627 - val_loss: 0.1674 - val_accuracy: 0.9474\n",
            "Epoch 466/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1047 - accuracy: 0.9591 - val_loss: 0.2107 - val_accuracy: 0.9123\n",
            "Epoch 467/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1494 - accuracy: 0.9307 - val_loss: 0.1638 - val_accuracy: 0.9035\n",
            "Epoch 468/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0803 - accuracy: 0.9700 - val_loss: 0.1675 - val_accuracy: 0.9474\n",
            "Epoch 469/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1249 - accuracy: 0.9597 - val_loss: 0.1676 - val_accuracy: 0.9474\n",
            "Epoch 470/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0823 - accuracy: 0.9685 - val_loss: 0.1639 - val_accuracy: 0.9386\n",
            "Epoch 471/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0800 - accuracy: 0.9691 - val_loss: 0.1968 - val_accuracy: 0.9123\n",
            "Epoch 472/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9780 - val_loss: 0.1848 - val_accuracy: 0.9123\n",
            "Epoch 473/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0856 - accuracy: 0.9666 - val_loss: 0.2075 - val_accuracy: 0.9123\n",
            "Epoch 474/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0985 - accuracy: 0.9490 - val_loss: 0.1615 - val_accuracy: 0.9035\n",
            "Epoch 475/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0653 - accuracy: 0.9780 - val_loss: 0.1614 - val_accuracy: 0.9211\n",
            "Epoch 476/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1192 - accuracy: 0.9468 - val_loss: 0.1690 - val_accuracy: 0.9298\n",
            "Epoch 477/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0919 - accuracy: 0.9679 - val_loss: 0.2163 - val_accuracy: 0.9298\n",
            "Epoch 478/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0840 - accuracy: 0.9702 - val_loss: 0.1555 - val_accuracy: 0.9386\n",
            "Epoch 479/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9663 - val_loss: 0.1516 - val_accuracy: 0.9386\n",
            "Epoch 480/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 0.9660 - val_loss: 0.1680 - val_accuracy: 0.9035\n",
            "Epoch 481/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0782 - accuracy: 0.9719 - val_loss: 0.1573 - val_accuracy: 0.9386\n",
            "Epoch 482/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1145 - accuracy: 0.9488 - val_loss: 0.1807 - val_accuracy: 0.9123\n",
            "Epoch 483/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1069 - accuracy: 0.9501 - val_loss: 0.1589 - val_accuracy: 0.9035\n",
            "Epoch 484/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0793 - accuracy: 0.9610 - val_loss: 0.2013 - val_accuracy: 0.9298\n",
            "Epoch 485/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0905 - accuracy: 0.9558 - val_loss: 0.1789 - val_accuracy: 0.9123\n",
            "Epoch 486/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1033 - accuracy: 0.9620 - val_loss: 0.1547 - val_accuracy: 0.9386\n",
            "Epoch 487/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1654 - accuracy: 0.9231 - val_loss: 0.1684 - val_accuracy: 0.9474\n",
            "Epoch 488/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1014 - accuracy: 0.9597 - val_loss: 0.1505 - val_accuracy: 0.9386\n",
            "Epoch 489/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0795 - accuracy: 0.9626 - val_loss: 0.1644 - val_accuracy: 0.9035\n",
            "Epoch 490/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0682 - accuracy: 0.9715 - val_loss: 0.1500 - val_accuracy: 0.9386\n",
            "Epoch 491/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1097 - accuracy: 0.9479 - val_loss: 0.2157 - val_accuracy: 0.9211\n",
            "Epoch 492/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9339 - val_loss: 0.2076 - val_accuracy: 0.9474\n",
            "Epoch 493/1000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0968 - accuracy: 0.9491 - val_loss: 0.2074 - val_accuracy: 0.9386\n",
            "Epoch 494/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1117 - accuracy: 0.9648 - val_loss: 0.1566 - val_accuracy: 0.9386\n",
            "Epoch 495/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0919 - accuracy: 0.9581 - val_loss: 0.1591 - val_accuracy: 0.9035\n",
            "Epoch 496/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.9714 - val_loss: 0.1612 - val_accuracy: 0.9123\n",
            "Epoch 497/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0720 - accuracy: 0.9692 - val_loss: 0.1788 - val_accuracy: 0.9211\n",
            "Epoch 498/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1038 - accuracy: 0.9538 - val_loss: 0.1656 - val_accuracy: 0.9386\n",
            "Epoch 499/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1297 - accuracy: 0.9559 - val_loss: 0.1706 - val_accuracy: 0.9123\n",
            "Epoch 500/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.1310 - accuracy: 0.9416 - val_loss: 0.1512 - val_accuracy: 0.9123\n",
            "Epoch 501/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1143 - accuracy: 0.9509 - val_loss: 0.1712 - val_accuracy: 0.9123\n",
            "Epoch 502/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0819 - accuracy: 0.9736 - val_loss: 0.1563 - val_accuracy: 0.9298\n",
            "Epoch 503/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0900 - accuracy: 0.9614 - val_loss: 0.1555 - val_accuracy: 0.9386\n",
            "Epoch 504/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9710 - val_loss: 0.1609 - val_accuracy: 0.9035\n",
            "Epoch 505/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0654 - accuracy: 0.9693 - val_loss: 0.1536 - val_accuracy: 0.9035\n",
            "Epoch 506/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0968 - accuracy: 0.9699 - val_loss: 0.1912 - val_accuracy: 0.9386\n",
            "Epoch 507/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1349 - accuracy: 0.9552 - val_loss: 0.1849 - val_accuracy: 0.9123\n",
            "Epoch 508/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1142 - accuracy: 0.9536 - val_loss: 0.1951 - val_accuracy: 0.9298\n",
            "Epoch 509/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1092 - accuracy: 0.9482 - val_loss: 0.1641 - val_accuracy: 0.9123\n",
            "Epoch 510/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0849 - accuracy: 0.9716 - val_loss: 0.1704 - val_accuracy: 0.9123\n",
            "Epoch 511/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0855 - accuracy: 0.9671 - val_loss: 0.1718 - val_accuracy: 0.9123\n",
            "Epoch 512/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.9715 - val_loss: 0.1455 - val_accuracy: 0.9386\n",
            "Epoch 513/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1020 - accuracy: 0.9551 - val_loss: 0.1543 - val_accuracy: 0.9035\n",
            "Epoch 514/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9615 - val_loss: 0.1741 - val_accuracy: 0.9123\n",
            "Epoch 515/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0799 - accuracy: 0.9658 - val_loss: 0.2135 - val_accuracy: 0.9211\n",
            "Epoch 516/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0663 - accuracy: 0.9725 - val_loss: 0.1461 - val_accuracy: 0.9298\n",
            "Epoch 517/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.9691 - val_loss: 0.1461 - val_accuracy: 0.9386\n",
            "Epoch 518/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.9718 - val_loss: 0.1591 - val_accuracy: 0.9386\n",
            "Epoch 519/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1185 - accuracy: 0.9654 - val_loss: 0.2891 - val_accuracy: 0.9035\n",
            "Epoch 520/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2011 - accuracy: 0.9343 - val_loss: 0.1584 - val_accuracy: 0.9211\n",
            "Epoch 521/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.9710 - val_loss: 0.1716 - val_accuracy: 0.9123\n",
            "Epoch 522/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1048 - accuracy: 0.9685 - val_loss: 0.1590 - val_accuracy: 0.9474\n",
            "Epoch 523/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1314 - accuracy: 0.9556 - val_loss: 0.1468 - val_accuracy: 0.9386\n",
            "Epoch 524/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1072 - accuracy: 0.9542 - val_loss: 0.1432 - val_accuracy: 0.9386\n",
            "Epoch 525/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1067 - accuracy: 0.9550 - val_loss: 0.1435 - val_accuracy: 0.9386\n",
            "Epoch 526/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 0.9765 - val_loss: 0.1597 - val_accuracy: 0.9123\n",
            "Epoch 527/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1167 - accuracy: 0.9547 - val_loss: 0.1942 - val_accuracy: 0.9298\n",
            "Epoch 528/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0968 - accuracy: 0.9592 - val_loss: 0.1503 - val_accuracy: 0.9298\n",
            "Epoch 529/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0762 - accuracy: 0.9728 - val_loss: 0.1461 - val_accuracy: 0.9123\n",
            "Epoch 530/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1130 - accuracy: 0.9604 - val_loss: 0.2153 - val_accuracy: 0.9123\n",
            "Epoch 531/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.9722 - val_loss: 0.1824 - val_accuracy: 0.9298\n",
            "Epoch 532/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1178 - accuracy: 0.9608 - val_loss: 0.1503 - val_accuracy: 0.9474\n",
            "Epoch 533/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.9644 - val_loss: 0.1398 - val_accuracy: 0.9386\n",
            "Epoch 534/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0913 - accuracy: 0.9707 - val_loss: 0.1614 - val_accuracy: 0.9123\n",
            "Epoch 535/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1038 - accuracy: 0.9692 - val_loss: 0.1582 - val_accuracy: 0.9474\n",
            "Epoch 536/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0956 - accuracy: 0.9576 - val_loss: 0.1617 - val_accuracy: 0.9123\n",
            "Epoch 537/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 0.9491 - val_loss: 0.1779 - val_accuracy: 0.9211\n",
            "Epoch 538/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.9747 - val_loss: 0.2032 - val_accuracy: 0.9298\n",
            "Epoch 539/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.1229 - accuracy: 0.9574 - val_loss: 0.1442 - val_accuracy: 0.9386\n",
            "Epoch 540/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1054 - accuracy: 0.9518 - val_loss: 0.1542 - val_accuracy: 0.9035\n",
            "Epoch 541/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0873 - accuracy: 0.9656 - val_loss: 0.1882 - val_accuracy: 0.9298\n",
            "Epoch 542/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0973 - accuracy: 0.9680 - val_loss: 0.1444 - val_accuracy: 0.9386\n",
            "Epoch 543/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9613 - val_loss: 0.1599 - val_accuracy: 0.9035\n",
            "Epoch 544/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9815 - val_loss: 0.1346 - val_accuracy: 0.9386\n",
            "Epoch 545/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0753 - accuracy: 0.9730 - val_loss: 0.1506 - val_accuracy: 0.9123\n",
            "Epoch 546/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1012 - accuracy: 0.9589 - val_loss: 0.1498 - val_accuracy: 0.9035\n",
            "Epoch 547/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9672 - val_loss: 0.1338 - val_accuracy: 0.9386\n",
            "Epoch 548/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9714 - val_loss: 0.1360 - val_accuracy: 0.9211\n",
            "Epoch 549/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0711 - accuracy: 0.9642 - val_loss: 0.1571 - val_accuracy: 0.9123\n",
            "Epoch 550/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1024 - accuracy: 0.9556 - val_loss: 0.1382 - val_accuracy: 0.9386\n",
            "Epoch 551/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.9639 - val_loss: 0.1474 - val_accuracy: 0.9035\n",
            "Epoch 552/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0701 - accuracy: 0.9699 - val_loss: 0.1431 - val_accuracy: 0.9386\n",
            "Epoch 553/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0801 - accuracy: 0.9741 - val_loss: 0.1438 - val_accuracy: 0.9035\n",
            "Epoch 554/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0850 - accuracy: 0.9700 - val_loss: 0.1344 - val_accuracy: 0.9211\n",
            "Epoch 555/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0848 - accuracy: 0.9650 - val_loss: 0.1611 - val_accuracy: 0.9123\n",
            "Epoch 556/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0636 - accuracy: 0.9830 - val_loss: 0.1485 - val_accuracy: 0.9123\n",
            "Epoch 557/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0750 - accuracy: 0.9587 - val_loss: 0.1348 - val_accuracy: 0.9386\n",
            "Epoch 558/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0985 - accuracy: 0.9564 - val_loss: 0.1400 - val_accuracy: 0.9298\n",
            "Epoch 559/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1114 - accuracy: 0.9396 - val_loss: 0.1664 - val_accuracy: 0.9123\n",
            "Epoch 560/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0794 - accuracy: 0.9675 - val_loss: 0.1369 - val_accuracy: 0.9123\n",
            "Epoch 561/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9697 - val_loss: 0.1345 - val_accuracy: 0.9386\n",
            "Epoch 562/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0869 - accuracy: 0.9656 - val_loss: 0.1523 - val_accuracy: 0.9474\n",
            "Epoch 563/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0899 - accuracy: 0.9592 - val_loss: 0.1308 - val_accuracy: 0.9386\n",
            "Epoch 564/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0796 - accuracy: 0.9695 - val_loss: 0.1320 - val_accuracy: 0.9386\n",
            "Epoch 565/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0846 - accuracy: 0.9635 - val_loss: 0.1847 - val_accuracy: 0.9298\n",
            "Epoch 566/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1348 - accuracy: 0.9420 - val_loss: 0.1924 - val_accuracy: 0.9298\n",
            "Epoch 567/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0947 - accuracy: 0.9640 - val_loss: 0.1463 - val_accuracy: 0.9123\n",
            "Epoch 568/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0873 - accuracy: 0.9610 - val_loss: 0.1398 - val_accuracy: 0.9211\n",
            "Epoch 569/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0707 - accuracy: 0.9678 - val_loss: 0.1637 - val_accuracy: 0.9386\n",
            "Epoch 570/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.9778 - val_loss: 0.1454 - val_accuracy: 0.9386\n",
            "Epoch 571/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1015 - accuracy: 0.9638 - val_loss: 0.1454 - val_accuracy: 0.9474\n",
            "Epoch 572/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0706 - accuracy: 0.9743 - val_loss: 0.1391 - val_accuracy: 0.9035\n",
            "Epoch 573/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0786 - accuracy: 0.9654 - val_loss: 0.1293 - val_accuracy: 0.9386\n",
            "Epoch 574/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0969 - accuracy: 0.9598 - val_loss: 0.1509 - val_accuracy: 0.9123\n",
            "Epoch 575/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0532 - accuracy: 0.9893 - val_loss: 0.2169 - val_accuracy: 0.9298\n",
            "Epoch 576/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.1523 - accuracy: 0.9443 - val_loss: 0.1319 - val_accuracy: 0.9386\n",
            "Epoch 577/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0951 - accuracy: 0.9598 - val_loss: 0.1298 - val_accuracy: 0.9386\n",
            "Epoch 578/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0782 - accuracy: 0.9548 - val_loss: 0.1311 - val_accuracy: 0.9386\n",
            "Epoch 579/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0809 - accuracy: 0.9741 - val_loss: 0.1442 - val_accuracy: 0.9123\n",
            "Epoch 580/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0989 - accuracy: 0.9608 - val_loss: 0.1299 - val_accuracy: 0.9123\n",
            "Epoch 581/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0717 - accuracy: 0.9675 - val_loss: 0.1456 - val_accuracy: 0.9123\n",
            "Epoch 582/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.9580 - val_loss: 0.1341 - val_accuracy: 0.9386\n",
            "Epoch 583/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1009 - accuracy: 0.9615 - val_loss: 0.1366 - val_accuracy: 0.9123\n",
            "Epoch 584/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1046 - accuracy: 0.9475 - val_loss: 0.1345 - val_accuracy: 0.9386\n",
            "Epoch 585/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0641 - accuracy: 0.9790 - val_loss: 0.1295 - val_accuracy: 0.9211\n",
            "Epoch 586/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0572 - accuracy: 0.9724 - val_loss: 0.1322 - val_accuracy: 0.9386\n",
            "Epoch 587/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0947 - accuracy: 0.9686 - val_loss: 0.1298 - val_accuracy: 0.9386\n",
            "Epoch 588/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 0.9620 - val_loss: 0.1781 - val_accuracy: 0.9298\n",
            "Epoch 589/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0917 - accuracy: 0.9686 - val_loss: 0.1431 - val_accuracy: 0.9123\n",
            "Epoch 590/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0767 - accuracy: 0.9740 - val_loss: 0.1570 - val_accuracy: 0.9123\n",
            "Epoch 591/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0895 - accuracy: 0.9649 - val_loss: 0.1305 - val_accuracy: 0.9386\n",
            "Epoch 592/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0846 - accuracy: 0.9782 - val_loss: 0.1312 - val_accuracy: 0.9386\n",
            "Epoch 593/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9739 - val_loss: 0.1543 - val_accuracy: 0.9474\n",
            "Epoch 594/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0792 - accuracy: 0.9705 - val_loss: 0.1251 - val_accuracy: 0.9386\n",
            "Epoch 595/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0772 - accuracy: 0.9731 - val_loss: 0.2033 - val_accuracy: 0.9386\n",
            "Epoch 596/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0884 - accuracy: 0.9671 - val_loss: 0.1309 - val_accuracy: 0.9386\n",
            "Epoch 597/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0974 - accuracy: 0.9654 - val_loss: 0.1466 - val_accuracy: 0.9474\n",
            "Epoch 598/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1213 - accuracy: 0.9508 - val_loss: 0.1377 - val_accuracy: 0.9123\n",
            "Epoch 599/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9622 - val_loss: 0.1335 - val_accuracy: 0.9386\n",
            "Epoch 600/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0764 - accuracy: 0.9710 - val_loss: 0.1307 - val_accuracy: 0.9298\n",
            "Epoch 601/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9654 - val_loss: 0.1811 - val_accuracy: 0.9298\n",
            "Epoch 602/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1038 - accuracy: 0.9502 - val_loss: 0.1682 - val_accuracy: 0.9561\n",
            "Epoch 603/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1166 - accuracy: 0.9435 - val_loss: 0.1223 - val_accuracy: 0.9386\n",
            "Epoch 604/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0732 - accuracy: 0.9652 - val_loss: 0.1239 - val_accuracy: 0.9298\n",
            "Epoch 605/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0767 - accuracy: 0.9738 - val_loss: 0.1905 - val_accuracy: 0.9298\n",
            "Epoch 606/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.9724 - val_loss: 0.1431 - val_accuracy: 0.9561\n",
            "Epoch 607/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1018 - accuracy: 0.9588 - val_loss: 0.1282 - val_accuracy: 0.9123\n",
            "Epoch 608/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0659 - accuracy: 0.9661 - val_loss: 0.1349 - val_accuracy: 0.9474\n",
            "Epoch 609/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1274 - accuracy: 0.9505 - val_loss: 0.3804 - val_accuracy: 0.9123\n",
            "Epoch 610/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.2230 - accuracy: 0.9224 - val_loss: 0.1375 - val_accuracy: 0.9474\n",
            "Epoch 611/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1206 - accuracy: 0.9615 - val_loss: 0.1326 - val_accuracy: 0.9474\n",
            "Epoch 612/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.9692 - val_loss: 0.1378 - val_accuracy: 0.9123\n",
            "Epoch 613/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0820 - accuracy: 0.9576 - val_loss: 0.1258 - val_accuracy: 0.9211\n",
            "Epoch 614/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0986 - accuracy: 0.9537 - val_loss: 0.1786 - val_accuracy: 0.9298\n",
            "Epoch 615/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1134 - accuracy: 0.9598 - val_loss: 0.1396 - val_accuracy: 0.9123\n",
            "Epoch 616/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1020 - accuracy: 0.9609 - val_loss: 0.1288 - val_accuracy: 0.9386\n",
            "Epoch 617/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0872 - accuracy: 0.9609 - val_loss: 0.1592 - val_accuracy: 0.9211\n",
            "Epoch 618/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 0.9599 - val_loss: 0.1217 - val_accuracy: 0.9386\n",
            "Epoch 619/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0772 - accuracy: 0.9702 - val_loss: 0.1660 - val_accuracy: 0.9298\n",
            "Epoch 620/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0679 - accuracy: 0.9727 - val_loss: 0.2633 - val_accuracy: 0.9123\n",
            "Epoch 621/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1148 - accuracy: 0.9498 - val_loss: 0.1667 - val_accuracy: 0.9211\n",
            "Epoch 622/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.9715 - val_loss: 0.1363 - val_accuracy: 0.9035\n",
            "Epoch 623/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0904 - accuracy: 0.9582 - val_loss: 0.1426 - val_accuracy: 0.9123\n",
            "Epoch 624/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0906 - accuracy: 0.9683 - val_loss: 0.1274 - val_accuracy: 0.9211\n",
            "Epoch 625/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.9693 - val_loss: 0.1709 - val_accuracy: 0.9211\n",
            "Epoch 626/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.9657 - val_loss: 0.1796 - val_accuracy: 0.9474\n",
            "Epoch 627/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1567 - accuracy: 0.9537 - val_loss: 0.1559 - val_accuracy: 0.9474\n",
            "Epoch 628/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1230 - accuracy: 0.9374 - val_loss: 0.1288 - val_accuracy: 0.9474\n",
            "Epoch 629/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0815 - accuracy: 0.9615 - val_loss: 0.2145 - val_accuracy: 0.9211\n",
            "Epoch 630/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1488 - accuracy: 0.9411 - val_loss: 0.1276 - val_accuracy: 0.9386\n",
            "Epoch 631/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0898 - accuracy: 0.9628 - val_loss: 0.1327 - val_accuracy: 0.9123\n",
            "Epoch 632/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0846 - accuracy: 0.9614 - val_loss: 0.1184 - val_accuracy: 0.9386\n",
            "Epoch 633/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0811 - accuracy: 0.9636 - val_loss: 0.1298 - val_accuracy: 0.9123\n",
            "Epoch 634/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0937 - accuracy: 0.9557 - val_loss: 0.1616 - val_accuracy: 0.9298\n",
            "Epoch 635/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0871 - accuracy: 0.9596 - val_loss: 0.1198 - val_accuracy: 0.9474\n",
            "Epoch 636/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0988 - accuracy: 0.9693 - val_loss: 0.2506 - val_accuracy: 0.9123\n",
            "Epoch 637/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1297 - accuracy: 0.9530 - val_loss: 0.1194 - val_accuracy: 0.9386\n",
            "Epoch 638/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0868 - accuracy: 0.9686 - val_loss: 0.1283 - val_accuracy: 0.9474\n",
            "Epoch 639/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0837 - accuracy: 0.9574 - val_loss: 0.1244 - val_accuracy: 0.9386\n",
            "Epoch 640/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0930 - accuracy: 0.9565 - val_loss: 0.1259 - val_accuracy: 0.9386\n",
            "Epoch 641/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1758 - accuracy: 0.9375 - val_loss: 0.2332 - val_accuracy: 0.9123\n",
            "Epoch 642/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1246 - accuracy: 0.9442 - val_loss: 0.1472 - val_accuracy: 0.9211\n",
            "Epoch 643/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0952 - accuracy: 0.9563 - val_loss: 0.1519 - val_accuracy: 0.9298\n",
            "Epoch 644/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0750 - accuracy: 0.9636 - val_loss: 0.1394 - val_accuracy: 0.9123\n",
            "Epoch 645/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0782 - accuracy: 0.9700 - val_loss: 0.2462 - val_accuracy: 0.9123\n",
            "Epoch 646/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1595 - accuracy: 0.9493 - val_loss: 0.1326 - val_accuracy: 0.9386\n",
            "Epoch 647/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1178 - accuracy: 0.9543 - val_loss: 0.1546 - val_accuracy: 0.9474\n",
            "Epoch 648/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0978 - accuracy: 0.9726 - val_loss: 0.1545 - val_accuracy: 0.9211\n",
            "Epoch 649/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.9629 - val_loss: 0.1606 - val_accuracy: 0.9298\n",
            "Epoch 650/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0893 - accuracy: 0.9648 - val_loss: 0.1343 - val_accuracy: 0.9386\n",
            "Epoch 651/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0779 - accuracy: 0.9716 - val_loss: 0.1584 - val_accuracy: 0.9474\n",
            "Epoch 652/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1261 - accuracy: 0.9527 - val_loss: 0.1615 - val_accuracy: 0.9298\n",
            "Epoch 653/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.9725 - val_loss: 0.1557 - val_accuracy: 0.9298\n",
            "Epoch 654/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0797 - accuracy: 0.9610 - val_loss: 0.1526 - val_accuracy: 0.9474\n",
            "Epoch 655/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0992 - accuracy: 0.9638 - val_loss: 0.1176 - val_accuracy: 0.9211\n",
            "Epoch 656/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0889 - accuracy: 0.9660 - val_loss: 0.1723 - val_accuracy: 0.9298\n",
            "Epoch 657/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0866 - accuracy: 0.9716 - val_loss: 0.1378 - val_accuracy: 0.9474\n",
            "Epoch 658/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.9629 - val_loss: 0.1224 - val_accuracy: 0.9386\n",
            "Epoch 659/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0944 - accuracy: 0.9714 - val_loss: 0.1225 - val_accuracy: 0.9474\n",
            "Epoch 660/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9725 - val_loss: 0.1230 - val_accuracy: 0.9474\n",
            "Epoch 661/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0748 - accuracy: 0.9772 - val_loss: 0.1179 - val_accuracy: 0.9386\n",
            "Epoch 662/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0979 - accuracy: 0.9701 - val_loss: 0.1277 - val_accuracy: 0.9211\n",
            "Epoch 663/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.9776 - val_loss: 0.1176 - val_accuracy: 0.9386\n",
            "Epoch 664/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0954 - accuracy: 0.9679 - val_loss: 0.1208 - val_accuracy: 0.9474\n",
            "Epoch 665/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0919 - accuracy: 0.9703 - val_loss: 0.1405 - val_accuracy: 0.9474\n",
            "Epoch 666/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0793 - accuracy: 0.9658 - val_loss: 0.1283 - val_accuracy: 0.9211\n",
            "Epoch 667/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0762 - accuracy: 0.9644 - val_loss: 0.1948 - val_accuracy: 0.9123\n",
            "Epoch 668/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0670 - accuracy: 0.9632 - val_loss: 0.2231 - val_accuracy: 0.9123\n",
            "Epoch 669/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.9620 - val_loss: 0.1162 - val_accuracy: 0.9211\n",
            "Epoch 670/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.9658 - val_loss: 0.1519 - val_accuracy: 0.9298\n",
            "Epoch 671/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0825 - accuracy: 0.9634 - val_loss: 0.1312 - val_accuracy: 0.9123\n",
            "Epoch 672/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0658 - accuracy: 0.9783 - val_loss: 0.1361 - val_accuracy: 0.9561\n",
            "Epoch 673/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1391 - accuracy: 0.9420 - val_loss: 0.1281 - val_accuracy: 0.9123\n",
            "Epoch 674/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0579 - accuracy: 0.9793 - val_loss: 0.1182 - val_accuracy: 0.9386\n",
            "Epoch 675/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0789 - accuracy: 0.9654 - val_loss: 0.1204 - val_accuracy: 0.9386\n",
            "Epoch 676/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0730 - accuracy: 0.9764 - val_loss: 0.1386 - val_accuracy: 0.9123\n",
            "Epoch 677/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0795 - accuracy: 0.9620 - val_loss: 0.1190 - val_accuracy: 0.9211\n",
            "Epoch 678/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0746 - accuracy: 0.9692 - val_loss: 0.2138 - val_accuracy: 0.9211\n",
            "Epoch 679/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0675 - accuracy: 0.9712 - val_loss: 0.1514 - val_accuracy: 0.9561\n",
            "Epoch 680/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0818 - accuracy: 0.9626 - val_loss: 0.1141 - val_accuracy: 0.9123\n",
            "Epoch 681/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 0.9641 - val_loss: 0.1825 - val_accuracy: 0.9386\n",
            "Epoch 682/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1241 - accuracy: 0.9644 - val_loss: 0.1401 - val_accuracy: 0.9123\n",
            "Epoch 683/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0978 - accuracy: 0.9668 - val_loss: 0.1181 - val_accuracy: 0.9386\n",
            "Epoch 684/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0948 - accuracy: 0.9579 - val_loss: 0.1296 - val_accuracy: 0.9123\n",
            "Epoch 685/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0674 - accuracy: 0.9860 - val_loss: 0.1310 - val_accuracy: 0.9474\n",
            "Epoch 686/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0797 - accuracy: 0.9654 - val_loss: 0.1237 - val_accuracy: 0.9123\n",
            "Epoch 687/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0758 - accuracy: 0.9635 - val_loss: 0.1178 - val_accuracy: 0.9386\n",
            "Epoch 688/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0970 - accuracy: 0.9540 - val_loss: 0.1229 - val_accuracy: 0.9123\n",
            "Epoch 689/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0633 - accuracy: 0.9788 - val_loss: 0.1509 - val_accuracy: 0.9474\n",
            "Epoch 690/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.9504 - val_loss: 0.1309 - val_accuracy: 0.9298\n",
            "Epoch 691/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9701 - val_loss: 0.1284 - val_accuracy: 0.9386\n",
            "Epoch 692/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0861 - accuracy: 0.9632 - val_loss: 0.1229 - val_accuracy: 0.9474\n",
            "Epoch 693/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0559 - accuracy: 0.9705 - val_loss: 0.1523 - val_accuracy: 0.9298\n",
            "Epoch 694/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9547 - val_loss: 0.1376 - val_accuracy: 0.9561\n",
            "Epoch 695/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0825 - accuracy: 0.9729 - val_loss: 0.1314 - val_accuracy: 0.9123\n",
            "Epoch 696/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.9695 - val_loss: 0.1438 - val_accuracy: 0.9123\n",
            "Epoch 697/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0726 - accuracy: 0.9631 - val_loss: 0.1403 - val_accuracy: 0.9561\n",
            "Epoch 698/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0707 - accuracy: 0.9678 - val_loss: 0.1201 - val_accuracy: 0.9298\n",
            "Epoch 699/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0713 - accuracy: 0.9643 - val_loss: 0.1692 - val_accuracy: 0.9298\n",
            "Epoch 700/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0713 - accuracy: 0.9832 - val_loss: 0.2772 - val_accuracy: 0.9211\n",
            "Epoch 701/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1226 - accuracy: 0.9620 - val_loss: 0.1191 - val_accuracy: 0.9386\n",
            "Epoch 702/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0909 - accuracy: 0.9617 - val_loss: 0.1604 - val_accuracy: 0.9298\n",
            "Epoch 703/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0723 - accuracy: 0.9653 - val_loss: 0.1276 - val_accuracy: 0.9123\n",
            "Epoch 704/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0608 - accuracy: 0.9710 - val_loss: 0.1552 - val_accuracy: 0.9474\n",
            "Epoch 705/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0694 - accuracy: 0.9754 - val_loss: 0.1176 - val_accuracy: 0.9386\n",
            "Epoch 706/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.9688 - val_loss: 0.1209 - val_accuracy: 0.9386\n",
            "Epoch 707/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0542 - accuracy: 0.9806 - val_loss: 0.1204 - val_accuracy: 0.9298\n",
            "Epoch 708/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0767 - accuracy: 0.9630 - val_loss: 0.1648 - val_accuracy: 0.9298\n",
            "Epoch 709/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.9805 - val_loss: 0.1438 - val_accuracy: 0.9211\n",
            "Epoch 710/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0830 - accuracy: 0.9585 - val_loss: 0.1305 - val_accuracy: 0.9123\n",
            "Epoch 711/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0433 - accuracy: 0.9856 - val_loss: 0.1264 - val_accuracy: 0.9386\n",
            "Epoch 712/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1047 - accuracy: 0.9415 - val_loss: 0.1739 - val_accuracy: 0.9298\n",
            "Epoch 713/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0905 - accuracy: 0.9551 - val_loss: 0.1271 - val_accuracy: 0.9211\n",
            "Epoch 714/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0773 - accuracy: 0.9644 - val_loss: 0.1184 - val_accuracy: 0.9386\n",
            "Epoch 715/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0560 - accuracy: 0.9747 - val_loss: 0.1563 - val_accuracy: 0.9298\n",
            "Epoch 716/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0835 - accuracy: 0.9669 - val_loss: 0.1653 - val_accuracy: 0.9298\n",
            "Epoch 717/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0536 - accuracy: 0.9795 - val_loss: 0.1776 - val_accuracy: 0.9298\n",
            "Epoch 718/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0796 - accuracy: 0.9608 - val_loss: 0.1398 - val_accuracy: 0.9474\n",
            "Epoch 719/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0720 - accuracy: 0.9705 - val_loss: 0.1301 - val_accuracy: 0.9123\n",
            "Epoch 720/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0617 - accuracy: 0.9729 - val_loss: 0.1164 - val_accuracy: 0.9386\n",
            "Epoch 721/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0922 - accuracy: 0.9677 - val_loss: 0.1259 - val_accuracy: 0.9386\n",
            "Epoch 722/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0944 - accuracy: 0.9679 - val_loss: 0.1157 - val_accuracy: 0.9211\n",
            "Epoch 723/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0767 - accuracy: 0.9589 - val_loss: 0.1317 - val_accuracy: 0.9561\n",
            "Epoch 724/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0933 - accuracy: 0.9540 - val_loss: 0.1206 - val_accuracy: 0.9123\n",
            "Epoch 725/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0609 - accuracy: 0.9791 - val_loss: 0.1502 - val_accuracy: 0.9211\n",
            "Epoch 726/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9662 - val_loss: 0.1441 - val_accuracy: 0.9123\n",
            "Epoch 727/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0828 - accuracy: 0.9592 - val_loss: 0.1734 - val_accuracy: 0.9298\n",
            "Epoch 728/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0893 - accuracy: 0.9628 - val_loss: 0.1180 - val_accuracy: 0.9123\n",
            "Epoch 729/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0680 - accuracy: 0.9658 - val_loss: 0.1339 - val_accuracy: 0.9123\n",
            "Epoch 730/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0807 - accuracy: 0.9574 - val_loss: 0.1925 - val_accuracy: 0.9298\n",
            "Epoch 731/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0606 - accuracy: 0.9753 - val_loss: 0.1214 - val_accuracy: 0.9386\n",
            "Epoch 732/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0937 - accuracy: 0.9559 - val_loss: 0.1955 - val_accuracy: 0.9211\n",
            "Epoch 733/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9578 - val_loss: 0.1218 - val_accuracy: 0.9386\n",
            "Epoch 734/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.9701 - val_loss: 0.1343 - val_accuracy: 0.9561\n",
            "Epoch 735/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0901 - accuracy: 0.9603 - val_loss: 0.1272 - val_accuracy: 0.9123\n",
            "Epoch 736/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0616 - accuracy: 0.9750 - val_loss: 0.1111 - val_accuracy: 0.9298\n",
            "Epoch 737/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0423 - accuracy: 0.9853 - val_loss: 0.1483 - val_accuracy: 0.9474\n",
            "Epoch 738/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0767 - accuracy: 0.9609 - val_loss: 0.1481 - val_accuracy: 0.9211\n",
            "Epoch 739/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 0.9560 - val_loss: 0.1219 - val_accuracy: 0.9386\n",
            "Epoch 740/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 0.9724 - val_loss: 0.1143 - val_accuracy: 0.9386\n",
            "Epoch 741/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0821 - accuracy: 0.9712 - val_loss: 0.1191 - val_accuracy: 0.9386\n",
            "Epoch 742/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0755 - accuracy: 0.9630 - val_loss: 0.1489 - val_accuracy: 0.9123\n",
            "Epoch 743/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0533 - accuracy: 0.9736 - val_loss: 0.1225 - val_accuracy: 0.9386\n",
            "Epoch 744/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.9706 - val_loss: 0.1251 - val_accuracy: 0.9474\n",
            "Epoch 745/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0757 - accuracy: 0.9589 - val_loss: 0.1538 - val_accuracy: 0.9211\n",
            "Epoch 746/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0666 - accuracy: 0.9746 - val_loss: 0.1258 - val_accuracy: 0.9474\n",
            "Epoch 747/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0943 - accuracy: 0.9589 - val_loss: 0.1277 - val_accuracy: 0.9298\n",
            "Epoch 748/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0886 - accuracy: 0.9691 - val_loss: 0.1448 - val_accuracy: 0.9298\n",
            "Epoch 749/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0512 - accuracy: 0.9721 - val_loss: 0.1205 - val_accuracy: 0.9298\n",
            "Epoch 750/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0631 - accuracy: 0.9835 - val_loss: 0.1196 - val_accuracy: 0.9123\n",
            "Epoch 751/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0758 - accuracy: 0.9683 - val_loss: 0.1212 - val_accuracy: 0.9123\n",
            "Epoch 752/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0712 - accuracy: 0.9661 - val_loss: 0.1216 - val_accuracy: 0.9386\n",
            "Epoch 753/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0729 - accuracy: 0.9668 - val_loss: 0.1330 - val_accuracy: 0.9123\n",
            "Epoch 754/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0629 - accuracy: 0.9678 - val_loss: 0.2218 - val_accuracy: 0.9211\n",
            "Epoch 755/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0511 - accuracy: 0.9795 - val_loss: 0.1635 - val_accuracy: 0.9211\n",
            "Epoch 756/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0833 - accuracy: 0.9656 - val_loss: 0.1124 - val_accuracy: 0.9386\n",
            "Epoch 757/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.9621 - val_loss: 0.1254 - val_accuracy: 0.9211\n",
            "Epoch 758/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.9678 - val_loss: 0.2263 - val_accuracy: 0.9298\n",
            "Epoch 759/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0616 - accuracy: 0.9722 - val_loss: 0.1082 - val_accuracy: 0.9298\n",
            "Epoch 760/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0695 - accuracy: 0.9635 - val_loss: 0.1391 - val_accuracy: 0.9386\n",
            "Epoch 761/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0669 - accuracy: 0.9721 - val_loss: 0.1249 - val_accuracy: 0.9123\n",
            "Epoch 762/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.9764 - val_loss: 0.1285 - val_accuracy: 0.9211\n",
            "Epoch 763/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0813 - accuracy: 0.9669 - val_loss: 0.1688 - val_accuracy: 0.9211\n",
            "Epoch 764/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0484 - accuracy: 0.9813 - val_loss: 0.2087 - val_accuracy: 0.9211\n",
            "Epoch 765/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0808 - accuracy: 0.9697 - val_loss: 0.2275 - val_accuracy: 0.9123\n",
            "Epoch 766/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1263 - accuracy: 0.9685 - val_loss: 0.1177 - val_accuracy: 0.9474\n",
            "Epoch 767/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.9815 - val_loss: 0.1405 - val_accuracy: 0.9561\n",
            "Epoch 768/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0907 - accuracy: 0.9658 - val_loss: 0.1189 - val_accuracy: 0.9561\n",
            "Epoch 769/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0832 - accuracy: 0.9662 - val_loss: 0.1137 - val_accuracy: 0.9211\n",
            "Epoch 770/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0595 - accuracy: 0.9635 - val_loss: 0.1177 - val_accuracy: 0.9211\n",
            "Epoch 771/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0513 - accuracy: 0.9739 - val_loss: 0.1120 - val_accuracy: 0.9386\n",
            "Epoch 772/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.9770 - val_loss: 0.1185 - val_accuracy: 0.9474\n",
            "Epoch 773/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0662 - accuracy: 0.9709 - val_loss: 0.1147 - val_accuracy: 0.9474\n",
            "Epoch 774/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0535 - accuracy: 0.9809 - val_loss: 0.1332 - val_accuracy: 0.9561\n",
            "Epoch 775/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0748 - accuracy: 0.9668 - val_loss: 0.1200 - val_accuracy: 0.9386\n",
            "Epoch 776/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0522 - accuracy: 0.9678 - val_loss: 0.1247 - val_accuracy: 0.9123\n",
            "Epoch 777/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0482 - accuracy: 0.9810 - val_loss: 0.1767 - val_accuracy: 0.9123\n",
            "Epoch 778/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0645 - accuracy: 0.9800 - val_loss: 0.1254 - val_accuracy: 0.9211\n",
            "Epoch 779/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.9563 - val_loss: 0.1171 - val_accuracy: 0.9561\n",
            "Epoch 780/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 0.9722 - val_loss: 0.1061 - val_accuracy: 0.9386\n",
            "Epoch 781/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0592 - accuracy: 0.9709 - val_loss: 0.1234 - val_accuracy: 0.9561\n",
            "Epoch 782/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0910 - accuracy: 0.9687 - val_loss: 0.1119 - val_accuracy: 0.9561\n",
            "Epoch 783/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0695 - accuracy: 0.9604 - val_loss: 0.1038 - val_accuracy: 0.9474\n",
            "Epoch 784/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0682 - accuracy: 0.9639 - val_loss: 0.1076 - val_accuracy: 0.9386\n",
            "Epoch 785/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0796 - accuracy: 0.9721 - val_loss: 0.1165 - val_accuracy: 0.9123\n",
            "Epoch 786/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0471 - accuracy: 0.9781 - val_loss: 0.1745 - val_accuracy: 0.9298\n",
            "Epoch 787/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0508 - accuracy: 0.9786 - val_loss: 0.1539 - val_accuracy: 0.9298\n",
            "Epoch 788/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.9544 - val_loss: 0.1275 - val_accuracy: 0.9386\n",
            "Epoch 789/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0624 - accuracy: 0.9751 - val_loss: 0.1151 - val_accuracy: 0.9298\n",
            "Epoch 790/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9662 - val_loss: 0.1172 - val_accuracy: 0.9298\n",
            "Epoch 791/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0818 - accuracy: 0.9642 - val_loss: 0.1187 - val_accuracy: 0.9298\n",
            "Epoch 792/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0515 - accuracy: 0.9739 - val_loss: 0.1494 - val_accuracy: 0.9298\n",
            "Epoch 793/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0526 - accuracy: 0.9755 - val_loss: 0.1542 - val_accuracy: 0.9386\n",
            "Epoch 794/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0830 - accuracy: 0.9596 - val_loss: 0.1109 - val_accuracy: 0.9386\n",
            "Epoch 795/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0678 - accuracy: 0.9736 - val_loss: 0.1320 - val_accuracy: 0.9298\n",
            "Epoch 796/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0432 - accuracy: 0.9816 - val_loss: 0.1072 - val_accuracy: 0.9298\n",
            "Epoch 797/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.9795 - val_loss: 0.1156 - val_accuracy: 0.9561\n",
            "Epoch 798/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0674 - accuracy: 0.9712 - val_loss: 0.1098 - val_accuracy: 0.9298\n",
            "Epoch 799/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0704 - accuracy: 0.9729 - val_loss: 0.1178 - val_accuracy: 0.9386\n",
            "Epoch 800/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0516 - accuracy: 0.9765 - val_loss: 0.1308 - val_accuracy: 0.9211\n",
            "Epoch 801/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0426 - accuracy: 0.9835 - val_loss: 0.1742 - val_accuracy: 0.9298\n",
            "Epoch 802/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0674 - accuracy: 0.9662 - val_loss: 0.1568 - val_accuracy: 0.9298\n",
            "Epoch 803/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0806 - accuracy: 0.9581 - val_loss: 0.1307 - val_accuracy: 0.9211\n",
            "Epoch 804/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0565 - accuracy: 0.9756 - val_loss: 0.1092 - val_accuracy: 0.9474\n",
            "Epoch 805/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0701 - accuracy: 0.9751 - val_loss: 0.1326 - val_accuracy: 0.9561\n",
            "Epoch 806/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0882 - accuracy: 0.9677 - val_loss: 0.1084 - val_accuracy: 0.9298\n",
            "Epoch 807/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0670 - accuracy: 0.9730 - val_loss: 0.1834 - val_accuracy: 0.9298\n",
            "Epoch 808/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0771 - accuracy: 0.9691 - val_loss: 0.1497 - val_accuracy: 0.9035\n",
            "Epoch 809/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0732 - accuracy: 0.9594 - val_loss: 0.1160 - val_accuracy: 0.9123\n",
            "Epoch 810/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.9772 - val_loss: 0.1443 - val_accuracy: 0.9298\n",
            "Epoch 811/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0481 - accuracy: 0.9845 - val_loss: 0.1286 - val_accuracy: 0.9211\n",
            "Epoch 812/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0494 - accuracy: 0.9701 - val_loss: 0.1351 - val_accuracy: 0.9123\n",
            "Epoch 813/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.9696 - val_loss: 0.1059 - val_accuracy: 0.9298\n",
            "Epoch 814/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0560 - accuracy: 0.9829 - val_loss: 0.1646 - val_accuracy: 0.9561\n",
            "Epoch 815/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0615 - accuracy: 0.9690 - val_loss: 0.1195 - val_accuracy: 0.9211\n",
            "Epoch 816/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0571 - accuracy: 0.9726 - val_loss: 0.1240 - val_accuracy: 0.9298\n",
            "Epoch 817/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0548 - accuracy: 0.9778 - val_loss: 0.1244 - val_accuracy: 0.9211\n",
            "Epoch 818/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.9668 - val_loss: 0.1226 - val_accuracy: 0.9298\n",
            "Epoch 819/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0706 - accuracy: 0.9676 - val_loss: 0.1131 - val_accuracy: 0.9035\n",
            "Epoch 820/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0491 - accuracy: 0.9853 - val_loss: 0.1127 - val_accuracy: 0.9386\n",
            "Epoch 821/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0825 - accuracy: 0.9685 - val_loss: 0.1123 - val_accuracy: 0.9386\n",
            "Epoch 822/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0553 - accuracy: 0.9746 - val_loss: 0.1899 - val_accuracy: 0.9298\n",
            "Epoch 823/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0987 - accuracy: 0.9681 - val_loss: 0.1174 - val_accuracy: 0.9386\n",
            "Epoch 824/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0540 - accuracy: 0.9726 - val_loss: 0.1173 - val_accuracy: 0.9474\n",
            "Epoch 825/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0479 - accuracy: 0.9797 - val_loss: 0.1366 - val_accuracy: 0.9386\n",
            "Epoch 826/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0699 - accuracy: 0.9592 - val_loss: 0.1035 - val_accuracy: 0.9386\n",
            "Epoch 827/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0550 - accuracy: 0.9793 - val_loss: 0.0924 - val_accuracy: 0.9386\n",
            "Epoch 828/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0453 - accuracy: 0.9815 - val_loss: 0.1116 - val_accuracy: 0.9298\n",
            "Epoch 829/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0532 - accuracy: 0.9750 - val_loss: 0.1067 - val_accuracy: 0.9298\n",
            "Epoch 830/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0429 - accuracy: 0.9842 - val_loss: 0.1373 - val_accuracy: 0.9386\n",
            "Epoch 831/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0675 - accuracy: 0.9735 - val_loss: 0.1506 - val_accuracy: 0.9474\n",
            "Epoch 832/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9628 - val_loss: 0.1185 - val_accuracy: 0.9298\n",
            "Epoch 833/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0507 - accuracy: 0.9775 - val_loss: 0.1059 - val_accuracy: 0.9386\n",
            "Epoch 834/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0998 - accuracy: 0.9697 - val_loss: 0.1477 - val_accuracy: 0.9474\n",
            "Epoch 835/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1305 - accuracy: 0.9530 - val_loss: 0.0996 - val_accuracy: 0.9298\n",
            "Epoch 836/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0427 - accuracy: 0.9766 - val_loss: 0.1580 - val_accuracy: 0.9474\n",
            "Epoch 837/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0742 - accuracy: 0.9692 - val_loss: 0.1179 - val_accuracy: 0.9474\n",
            "Epoch 838/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0458 - accuracy: 0.9790 - val_loss: 0.1447 - val_accuracy: 0.9561\n",
            "Epoch 839/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0817 - accuracy: 0.9569 - val_loss: 0.0952 - val_accuracy: 0.9474\n",
            "Epoch 840/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0718 - accuracy: 0.9762 - val_loss: 0.0959 - val_accuracy: 0.9474\n",
            "Epoch 841/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0457 - accuracy: 0.9821 - val_loss: 0.2505 - val_accuracy: 0.9123\n",
            "Epoch 842/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1156 - accuracy: 0.9652 - val_loss: 0.1300 - val_accuracy: 0.9298\n",
            "Epoch 843/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0614 - accuracy: 0.9641 - val_loss: 0.1278 - val_accuracy: 0.9474\n",
            "Epoch 844/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.9683 - val_loss: 0.1085 - val_accuracy: 0.9386\n",
            "Epoch 845/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0586 - accuracy: 0.9756 - val_loss: 0.1479 - val_accuracy: 0.9298\n",
            "Epoch 846/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9717 - val_loss: 0.0944 - val_accuracy: 0.9386\n",
            "Epoch 847/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0338 - accuracy: 0.9895 - val_loss: 0.1291 - val_accuracy: 0.9386\n",
            "Epoch 848/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0708 - accuracy: 0.9585 - val_loss: 0.1451 - val_accuracy: 0.9474\n",
            "Epoch 849/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0309 - accuracy: 0.9861 - val_loss: 0.1392 - val_accuracy: 0.9561\n",
            "Epoch 850/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0822 - accuracy: 0.9624 - val_loss: 0.0928 - val_accuracy: 0.9474\n",
            "Epoch 851/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0629 - accuracy: 0.9720 - val_loss: 0.0902 - val_accuracy: 0.9386\n",
            "Epoch 852/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0530 - accuracy: 0.9762 - val_loss: 0.1532 - val_accuracy: 0.9298\n",
            "Epoch 853/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0483 - accuracy: 0.9798 - val_loss: 0.1092 - val_accuracy: 0.9211\n",
            "Epoch 854/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0652 - accuracy: 0.9635 - val_loss: 0.1286 - val_accuracy: 0.9386\n",
            "Epoch 855/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1033 - accuracy: 0.9536 - val_loss: 0.0923 - val_accuracy: 0.9298\n",
            "Epoch 856/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0654 - accuracy: 0.9701 - val_loss: 0.1207 - val_accuracy: 0.9123\n",
            "Epoch 857/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0457 - accuracy: 0.9781 - val_loss: 0.1113 - val_accuracy: 0.9474\n",
            "Epoch 858/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0464 - accuracy: 0.9775 - val_loss: 0.1315 - val_accuracy: 0.9211\n",
            "Epoch 859/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0627 - accuracy: 0.9723 - val_loss: 0.1133 - val_accuracy: 0.9474\n",
            "Epoch 860/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0533 - accuracy: 0.9679 - val_loss: 0.1478 - val_accuracy: 0.9211\n",
            "Epoch 861/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0773 - accuracy: 0.9566 - val_loss: 0.1294 - val_accuracy: 0.9211\n",
            "Epoch 862/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0652 - accuracy: 0.9689 - val_loss: 0.0954 - val_accuracy: 0.9386\n",
            "Epoch 863/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0456 - accuracy: 0.9788 - val_loss: 0.1601 - val_accuracy: 0.9474\n",
            "Epoch 864/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9790 - val_loss: 0.1537 - val_accuracy: 0.9386\n",
            "Epoch 865/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0695 - accuracy: 0.9711 - val_loss: 0.0928 - val_accuracy: 0.9474\n",
            "Epoch 866/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0463 - accuracy: 0.9768 - val_loss: 0.1070 - val_accuracy: 0.9474\n",
            "Epoch 867/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0479 - accuracy: 0.9743 - val_loss: 0.1730 - val_accuracy: 0.9298\n",
            "Epoch 868/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0568 - accuracy: 0.9740 - val_loss: 0.1072 - val_accuracy: 0.9474\n",
            "Epoch 869/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.9696 - val_loss: 0.1058 - val_accuracy: 0.9474\n",
            "Epoch 870/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0568 - accuracy: 0.9762 - val_loss: 0.1326 - val_accuracy: 0.9386\n",
            "Epoch 871/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.9810 - val_loss: 0.1244 - val_accuracy: 0.9298\n",
            "Epoch 872/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0361 - accuracy: 0.9875 - val_loss: 0.1085 - val_accuracy: 0.9211\n",
            "Epoch 873/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.9780 - val_loss: 0.1372 - val_accuracy: 0.9211\n",
            "Epoch 874/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0499 - accuracy: 0.9777 - val_loss: 0.1391 - val_accuracy: 0.9386\n",
            "Epoch 875/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0855 - accuracy: 0.9663 - val_loss: 0.1118 - val_accuracy: 0.9386\n",
            "Epoch 876/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0470 - accuracy: 0.9688 - val_loss: 0.1414 - val_accuracy: 0.9211\n",
            "Epoch 877/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0506 - accuracy: 0.9821 - val_loss: 0.1124 - val_accuracy: 0.9211\n",
            "Epoch 878/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0547 - accuracy: 0.9704 - val_loss: 0.1142 - val_accuracy: 0.9386\n",
            "Epoch 879/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0884 - accuracy: 0.9566 - val_loss: 0.1002 - val_accuracy: 0.9298\n",
            "Epoch 880/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0460 - accuracy: 0.9802 - val_loss: 0.1345 - val_accuracy: 0.9474\n",
            "Epoch 881/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0454 - accuracy: 0.9871 - val_loss: 0.0881 - val_accuracy: 0.9474\n",
            "Epoch 882/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0467 - accuracy: 0.9807 - val_loss: 0.1008 - val_accuracy: 0.9298\n",
            "Epoch 883/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0633 - accuracy: 0.9781 - val_loss: 0.0980 - val_accuracy: 0.9298\n",
            "Epoch 884/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0511 - accuracy: 0.9735 - val_loss: 0.1011 - val_accuracy: 0.9561\n",
            "Epoch 885/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0430 - accuracy: 0.9823 - val_loss: 0.1125 - val_accuracy: 0.9211\n",
            "Epoch 886/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0435 - accuracy: 0.9824 - val_loss: 0.0962 - val_accuracy: 0.9386\n",
            "Epoch 887/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0494 - accuracy: 0.9850 - val_loss: 0.1155 - val_accuracy: 0.9298\n",
            "Epoch 888/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0506 - accuracy: 0.9822 - val_loss: 0.0938 - val_accuracy: 0.9474\n",
            "Epoch 889/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.9665 - val_loss: 0.1166 - val_accuracy: 0.9211\n",
            "Epoch 890/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0681 - accuracy: 0.9705 - val_loss: 0.1439 - val_accuracy: 0.9211\n",
            "Epoch 891/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0809 - accuracy: 0.9676 - val_loss: 0.0941 - val_accuracy: 0.9386\n",
            "Epoch 892/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0572 - accuracy: 0.9787 - val_loss: 0.0991 - val_accuracy: 0.9386\n",
            "Epoch 893/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0628 - accuracy: 0.9729 - val_loss: 0.0984 - val_accuracy: 0.9211\n",
            "Epoch 894/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0639 - accuracy: 0.9678 - val_loss: 0.1244 - val_accuracy: 0.9561\n",
            "Epoch 895/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0909 - accuracy: 0.9722 - val_loss: 0.1083 - val_accuracy: 0.9211\n",
            "Epoch 896/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0656 - accuracy: 0.9746 - val_loss: 0.1306 - val_accuracy: 0.9211\n",
            "Epoch 897/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0455 - accuracy: 0.9807 - val_loss: 0.0864 - val_accuracy: 0.9474\n",
            "Epoch 898/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0369 - accuracy: 0.9872 - val_loss: 0.0989 - val_accuracy: 0.9386\n",
            "Epoch 899/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0826 - accuracy: 0.9665 - val_loss: 0.1832 - val_accuracy: 0.9474\n",
            "Epoch 900/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0618 - accuracy: 0.9806 - val_loss: 0.0897 - val_accuracy: 0.9474\n",
            "Epoch 901/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0461 - accuracy: 0.9715 - val_loss: 0.1006 - val_accuracy: 0.9386\n",
            "Epoch 902/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0390 - accuracy: 0.9820 - val_loss: 0.0944 - val_accuracy: 0.9561\n",
            "Epoch 903/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0437 - accuracy: 0.9817 - val_loss: 0.0955 - val_accuracy: 0.9474\n",
            "Epoch 904/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0837 - accuracy: 0.9655 - val_loss: 0.1908 - val_accuracy: 0.9561\n",
            "Epoch 905/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1063 - accuracy: 0.9672 - val_loss: 0.0867 - val_accuracy: 0.9474\n",
            "Epoch 906/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0760 - accuracy: 0.9728 - val_loss: 0.0798 - val_accuracy: 0.9474\n",
            "Epoch 907/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0556 - accuracy: 0.9702 - val_loss: 0.0935 - val_accuracy: 0.9386\n",
            "Epoch 908/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.9654 - val_loss: 0.0913 - val_accuracy: 0.9386\n",
            "Epoch 909/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0464 - accuracy: 0.9787 - val_loss: 0.0929 - val_accuracy: 0.9474\n",
            "Epoch 910/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0522 - accuracy: 0.9754 - val_loss: 0.0922 - val_accuracy: 0.9474\n",
            "Epoch 911/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0507 - accuracy: 0.9781 - val_loss: 0.1723 - val_accuracy: 0.9386\n",
            "Epoch 912/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0591 - accuracy: 0.9734 - val_loss: 0.1365 - val_accuracy: 0.9035\n",
            "Epoch 913/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0456 - accuracy: 0.9827 - val_loss: 0.1196 - val_accuracy: 0.9298\n",
            "Epoch 914/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0530 - accuracy: 0.9743 - val_loss: 0.1125 - val_accuracy: 0.9386\n",
            "Epoch 915/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0526 - accuracy: 0.9750 - val_loss: 0.1385 - val_accuracy: 0.9211\n",
            "Epoch 916/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0435 - accuracy: 0.9745 - val_loss: 0.1201 - val_accuracy: 0.9298\n",
            "Epoch 917/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0479 - accuracy: 0.9894 - val_loss: 0.1941 - val_accuracy: 0.9474\n",
            "Epoch 918/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0717 - accuracy: 0.9757 - val_loss: 0.1954 - val_accuracy: 0.9298\n",
            "Epoch 919/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0820 - accuracy: 0.9725 - val_loss: 0.1660 - val_accuracy: 0.9474\n",
            "Epoch 920/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0508 - accuracy: 0.9793 - val_loss: 0.1048 - val_accuracy: 0.9561\n",
            "Epoch 921/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0416 - accuracy: 0.9854 - val_loss: 0.0953 - val_accuracy: 0.9474\n",
            "Epoch 922/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0454 - accuracy: 0.9822 - val_loss: 0.1182 - val_accuracy: 0.9298\n",
            "Epoch 923/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0520 - accuracy: 0.9808 - val_loss: 0.1247 - val_accuracy: 0.9211\n",
            "Epoch 924/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0372 - accuracy: 0.9853 - val_loss: 0.0956 - val_accuracy: 0.9298\n",
            "Epoch 925/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0345 - accuracy: 0.9862 - val_loss: 0.1861 - val_accuracy: 0.9474\n",
            "Epoch 926/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 0.9797 - val_loss: 0.1728 - val_accuracy: 0.9298\n",
            "Epoch 927/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0433 - accuracy: 0.9823 - val_loss: 0.1179 - val_accuracy: 0.9561\n",
            "Epoch 928/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0521 - accuracy: 0.9744 - val_loss: 0.1141 - val_accuracy: 0.9561\n",
            "Epoch 929/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0598 - accuracy: 0.9773 - val_loss: 0.1064 - val_accuracy: 0.9474\n",
            "Epoch 930/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 0.9764 - val_loss: 0.1449 - val_accuracy: 0.9386\n",
            "Epoch 931/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0432 - accuracy: 0.9818 - val_loss: 0.0963 - val_accuracy: 0.9474\n",
            "Epoch 932/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0441 - accuracy: 0.9807 - val_loss: 0.0950 - val_accuracy: 0.9474\n",
            "Epoch 933/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0322 - accuracy: 0.9902 - val_loss: 0.1225 - val_accuracy: 0.9474\n",
            "Epoch 934/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0418 - accuracy: 0.9718 - val_loss: 0.1230 - val_accuracy: 0.9211\n",
            "Epoch 935/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0317 - accuracy: 0.9857 - val_loss: 0.0851 - val_accuracy: 0.9561\n",
            "Epoch 936/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.9851 - val_loss: 0.1112 - val_accuracy: 0.9298\n",
            "Epoch 937/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9839 - val_loss: 0.0958 - val_accuracy: 0.9561\n",
            "Epoch 938/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0491 - accuracy: 0.9747 - val_loss: 0.2024 - val_accuracy: 0.9298\n",
            "Epoch 939/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0310 - accuracy: 0.9860 - val_loss: 0.1109 - val_accuracy: 0.9211\n",
            "Epoch 940/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0578 - accuracy: 0.9654 - val_loss: 0.1371 - val_accuracy: 0.9386\n",
            "Epoch 941/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0558 - accuracy: 0.9730 - val_loss: 0.1003 - val_accuracy: 0.9298\n",
            "Epoch 942/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0707 - accuracy: 0.9654 - val_loss: 0.0814 - val_accuracy: 0.9474\n",
            "Epoch 943/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0487 - accuracy: 0.9790 - val_loss: 0.0907 - val_accuracy: 0.9298\n",
            "Epoch 944/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0409 - accuracy: 0.9793 - val_loss: 0.0895 - val_accuracy: 0.9561\n",
            "Epoch 945/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0458 - accuracy: 0.9771 - val_loss: 0.1398 - val_accuracy: 0.9474\n",
            "Epoch 946/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0446 - accuracy: 0.9815 - val_loss: 0.1897 - val_accuracy: 0.9474\n",
            "Epoch 947/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0860 - accuracy: 0.9675 - val_loss: 0.0982 - val_accuracy: 0.9386\n",
            "Epoch 948/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0836 - accuracy: 0.9537 - val_loss: 0.0775 - val_accuracy: 0.9474\n",
            "Epoch 949/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0747 - accuracy: 0.9595 - val_loss: 0.1270 - val_accuracy: 0.9298\n",
            "Epoch 950/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.9702 - val_loss: 0.1103 - val_accuracy: 0.9298\n",
            "Epoch 951/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0503 - accuracy: 0.9780 - val_loss: 0.1136 - val_accuracy: 0.9211\n",
            "Epoch 952/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0269 - accuracy: 0.9902 - val_loss: 0.1542 - val_accuracy: 0.9474\n",
            "Epoch 953/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0830 - accuracy: 0.9674 - val_loss: 0.1305 - val_accuracy: 0.9474\n",
            "Epoch 954/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0726 - accuracy: 0.9651 - val_loss: 0.0972 - val_accuracy: 0.9474\n",
            "Epoch 955/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0572 - accuracy: 0.9736 - val_loss: 0.1326 - val_accuracy: 0.9474\n",
            "Epoch 956/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0490 - accuracy: 0.9846 - val_loss: 0.0793 - val_accuracy: 0.9474\n",
            "Epoch 957/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 0.9915 - val_loss: 0.0931 - val_accuracy: 0.9386\n",
            "Epoch 958/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 0.9703 - val_loss: 0.1070 - val_accuracy: 0.9474\n",
            "Epoch 959/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 0.9851 - val_loss: 0.0940 - val_accuracy: 0.9298\n",
            "Epoch 960/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0376 - accuracy: 0.9831 - val_loss: 0.1226 - val_accuracy: 0.9211\n",
            "Epoch 961/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0466 - accuracy: 0.9777 - val_loss: 0.1082 - val_accuracy: 0.9474\n",
            "Epoch 962/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0433 - accuracy: 0.9815 - val_loss: 0.1279 - val_accuracy: 0.9298\n",
            "Epoch 963/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0498 - accuracy: 0.9794 - val_loss: 0.0880 - val_accuracy: 0.9474\n",
            "Epoch 964/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0487 - accuracy: 0.9757 - val_loss: 0.0934 - val_accuracy: 0.9474\n",
            "Epoch 965/1000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.0926 - accuracy: 0.9605 - val_loss: 0.1187 - val_accuracy: 0.9211\n",
            "Epoch 966/1000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.1006 - accuracy: 0.9660 - val_loss: 0.1826 - val_accuracy: 0.9211\n",
            "Epoch 967/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.9726 - val_loss: 0.1622 - val_accuracy: 0.9211\n",
            "Epoch 968/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0472 - accuracy: 0.9835 - val_loss: 0.1210 - val_accuracy: 0.9474\n",
            "Epoch 969/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0671 - accuracy: 0.9716 - val_loss: 0.1010 - val_accuracy: 0.9386\n",
            "Epoch 970/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0399 - accuracy: 0.9841 - val_loss: 0.1143 - val_accuracy: 0.9211\n",
            "Epoch 971/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9774 - val_loss: 0.0900 - val_accuracy: 0.9386\n",
            "Epoch 972/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0416 - accuracy: 0.9782 - val_loss: 0.0929 - val_accuracy: 0.9298\n",
            "Epoch 973/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0413 - accuracy: 0.9791 - val_loss: 0.0909 - val_accuracy: 0.9561\n",
            "Epoch 974/1000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.0329 - accuracy: 0.9825 - val_loss: 0.1463 - val_accuracy: 0.9561\n",
            "Epoch 975/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.1110 - accuracy: 0.9632 - val_loss: 0.1145 - val_accuracy: 0.9298\n",
            "Epoch 976/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0506 - accuracy: 0.9743 - val_loss: 0.1798 - val_accuracy: 0.9474\n",
            "Epoch 977/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0813 - accuracy: 0.9691 - val_loss: 0.1405 - val_accuracy: 0.9386\n",
            "Epoch 978/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0698 - accuracy: 0.9703 - val_loss: 0.2010 - val_accuracy: 0.9298\n",
            "Epoch 979/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0742 - accuracy: 0.9667 - val_loss: 0.1280 - val_accuracy: 0.9561\n",
            "Epoch 980/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.9770 - val_loss: 0.0880 - val_accuracy: 0.9561\n",
            "Epoch 981/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9862 - val_loss: 0.0809 - val_accuracy: 0.9474\n",
            "Epoch 982/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0418 - accuracy: 0.9759 - val_loss: 0.0975 - val_accuracy: 0.9211\n",
            "Epoch 983/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 0.9786 - val_loss: 0.1043 - val_accuracy: 0.9298\n",
            "Epoch 984/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 0.9809 - val_loss: 0.1047 - val_accuracy: 0.9474\n",
            "Epoch 985/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.9844 - val_loss: 0.0878 - val_accuracy: 0.9561\n",
            "Epoch 986/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0323 - accuracy: 0.9864 - val_loss: 0.1144 - val_accuracy: 0.9386\n",
            "Epoch 987/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0452 - accuracy: 0.9851 - val_loss: 0.0888 - val_accuracy: 0.9474\n",
            "Epoch 988/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0372 - accuracy: 0.9893 - val_loss: 0.1305 - val_accuracy: 0.9386\n",
            "Epoch 989/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9639 - val_loss: 0.1834 - val_accuracy: 0.9211\n",
            "Epoch 990/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0768 - accuracy: 0.9779 - val_loss: 0.0805 - val_accuracy: 0.9298\n",
            "Epoch 991/1000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 0.0408 - accuracy: 0.9783 - val_loss: 0.0820 - val_accuracy: 0.9474\n",
            "Epoch 992/1000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.0855 - accuracy: 0.9648 - val_loss: 0.1355 - val_accuracy: 0.9474\n",
            "Epoch 993/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0879 - accuracy: 0.9649 - val_loss: 0.0881 - val_accuracy: 0.9474\n",
            "Epoch 994/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.9828 - val_loss: 0.0924 - val_accuracy: 0.9561\n",
            "Epoch 995/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0614 - accuracy: 0.9820 - val_loss: 0.0804 - val_accuracy: 0.9474\n",
            "Epoch 996/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0937 - accuracy: 0.9681 - val_loss: 0.0785 - val_accuracy: 0.9561\n",
            "Epoch 997/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.9725 - val_loss: 0.0917 - val_accuracy: 0.9386\n",
            "Epoch 998/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.9621 - val_loss: 0.0808 - val_accuracy: 0.9561\n",
            "Epoch 999/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 0.9753 - val_loss: 0.0739 - val_accuracy: 0.9561\n",
            "Epoch 1000/1000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.9745 - val_loss: 0.0832 - val_accuracy: 0.9474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "-SOALOs_6-IY",
        "outputId": "0188f78a-3b33-4b3d-c0c9-11d5d3affca8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(H.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(H.history['accuracy'])\n",
        "plt.plot(H.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(H.history['loss'])\n",
        "plt.plot(H.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyjLvi1l7CsX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}